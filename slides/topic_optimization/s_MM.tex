% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  14pt,
  ignorenonframetext,
  aspectratio=169,
]{beamer}
\newif\ifbibliography
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% remove section numbering
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{section title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{subsection title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}

\usetheme[]{metropolis}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother


\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
\usepackage{caption}
% Make caption package work with longtable
\makeatletter
\def\fnum@table{\tablename~\thetable}
\makeatother
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\usepackage{xcolor}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={The EM algorithm II: theory and inference},
  pdfauthor={Julia Wrobel},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}


\title{The EM algorithm II: theory and inference}
\author{Julia Wrobel}
\date{}

\begin{document}
\frame{\titlepage}


\begin{frame}{Overview}
\phantomsection\label{overview}
\fontsize{10pt}{11pt}\selectfont

Today, we cover:

\begin{itemize}
\tightlist
\item
  The MM Algorithm
\end{itemize}

Announcements

\begin{itemize}
\tightlist
\item
  HW3 posted and due 3/4 at 10:00AM
\end{itemize}

Readings:

\begin{itemize}
\tightlist
\item
  Hunter and Lange: A tutorial on MM algorithms, \emph{The American
  Statistician}
\end{itemize}

\note{Tutorial article is very concise but overall pretty good if you
want to learn more.}
\end{frame}

\begin{frame}{EM as MM}
\phantomsection\label{em-as-mm}
The EM is a \textbf{minorization} approach. Instead of directly
maximizing. the log-likelihood, which is hard, the algorithm constructs
a minorizing function and optimizes that function instead.

A function \(g\) \emph{minorizes} \(f\) over \(\mathcal{X}\) at \(y\)
if:

\begin{enumerate}
\tightlist
\item
  \(g(x) \le f(x) \mbox{ for all } x \in \mathcal{X}\)
\item
  \(g(y) = f(y)\)
\end{enumerate}
\end{frame}

\begin{frame}{MM algorithm}
\phantomsection\label{mm-algorithm}
\begin{itemize}
\item
  Stands for ``Majorize-Minimization'' or ``Minorize-Maximization'',
  depending on whether the desired optimization is a minimization or a
  maximization
\item
  Not actually an algorithm, but a strategy for constructing
  optimization algorithms
\item
  EM is a special case
\end{itemize}
\end{frame}

\begin{frame}{MM algorithm}
\phantomsection\label{mm-algorithm-1}
\begin{itemize}
\item
  Stands for ``Majorize-Minimization'' or
  ``Minorize-Maximization'',depending on whether the desired
  optimization is a minimization or a maximization
\item
  Not actually an algorithm, but a strategy for constructing
  optimization algorithms
\item
  EM is a special case
\end{itemize}

\textbf{Idea}: MM algorithm operates by creating a \textbf{surrogate
function} that minorizes or majorizes the objective function. When the
surrogate function is optimized, the objective function is driven uphill
or downhill as needed.
\end{frame}

\begin{frame}{Definition of an MM algorithm for Minimization}
\phantomsection\label{definition-of-an-mm-algorithm-for-minimization}
We first focus on the \textbf{minimization} problem, in which MM =
Majorize--Minimize.

\begin{itemize}
\tightlist
\item
  A function \(g(\theta|\theta^t)\) is said to \textbf{majorize} the
  function \(f(\theta)\) at \(\theta^t\) if
\end{itemize}

\begin{align}
f(\theta) &\le g(\theta|\theta^t) \mbox{ for all } \theta\\[2mm]
f(\theta^t) &= g(\theta^t|\theta^t)
\end{align}

\begin{itemize}
\item
  We choose a majorizing function \(g(\theta|\theta^t)\) and
  \textbf{minimize} it, instead of minimizing \(f(\theta)\). Denote
  \(\theta^{t+1} =\arg\min_{\theta}g(\theta|\theta^t)\). Iterate until
  \(\theta^t\) converges.
\item
  \textbf{Descent property}:
  \(f(\theta^t) \le g(\theta^{t+1}|\theta^t)\le g(\theta^t|\theta^t) = f(\theta^t)\)
\end{itemize}

\note{I think pictures really help here}
\end{frame}

\begin{frame}{Definition of an MM algorithm for Minimization}
\phantomsection\label{definition-of-an-mm-algorithm-for-minimization-1}
\begin{center}
\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{mm.png}
\end{center}

\note{Two examples: Dotted line here is the majorizing function}
\end{frame}

\begin{frame}{MM algorithm for Maximization}
\phantomsection\label{mm-algorithm-for-maximization}
In a \textbf{maximization} problem, MM = Minorize--Maximize.

\begin{itemize}
\item
  To maximize \(f(\theta)\), we \textbf{minorize} it by a surrogate
  function \(g(\theta|\theta^t)\) and maximize \(g(\theta|\theta^t)\) to
  produce the next iteration \(\theta^{t+1}\)
\item
  \(g(\theta|\theta^t)\) minorizes \(f(\theta)\) at \(\theta^t\) if
  \(-g(\theta|\theta^t)\) majorizes \(-f(\theta)\)
\end{itemize}

\begin{center}
\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{mm2.png}
\end{center}
\end{frame}

\begin{frame}{Separation of high-dimensional parameter spaces}
\phantomsection\label{separation-of-high-dimensional-parameter-spaces}
One of the key criteria in judging majorizing or minorizing functions is
their \textbf{ease of optimization}.

\begin{itemize}
\tightlist
\item
  Successful MM algorithms in high-dimensional parameter spaces often
  rely on surrogate functions in which the individual parameter
  components are \textbf{separated}, i.e., for
  \(\theta = (\theta_1, \ldots, \theta_p)\),
\end{itemize}

\[g(\theta|\theta^t) = \sum_{j = 1}^p q_j(\theta_j)\] where
\(q_j(\cdot)\) are univariate functions.

Because the \(p\) univariate functions may be \textbf{optimized one by
one}, this makes the surrogate function easier to optimize at each
iteration.
\end{frame}

\begin{frame}{Advantages of the MM algorithm}
\phantomsection\label{advantages-of-the-mm-algorithm}
\begin{itemize}
\item
  \textbf{Numerical stability}: warranted by the descent (or ascent)
  property
\item
  \textbf{Simplicity}: Turn a difficult optimization problem into a
  simple one

  \begin{itemize}
  \tightlist
  \item
    It can turn a non-differentiable problem into a smooth problem
    (Example 2).
  \item
    It can separate the parameters of a problem (Example 3).
  \item
    It can linearize an optimization problem (Example 3).
  \item
    It can deal gracefully with equality and inequality constraints
    (Example 4).
  \item
    It can generate an algorithm that avoids large matrix inversion (5).
  \end{itemize}
\item
  Iteration is the price we pay for simplifying the original problem.
\end{itemize}

\note{Seems like magic. How do you actually construct the surrogate
function?}
\end{frame}

\begin{frame}{EM algorithm vs.~MM algorithm}
\phantomsection\label{em-algorithm-vs.-mm-algorithm}
\begin{itemize}
\item
  \textbf{EM}: The E-step creates a surrogate function by identifying a
  complete-data log-likelihood function and evaluating it with respect
  to the observed data. The M-step maximizes the surrogate function.
  Every EM algorithm is an example of an MM algorithm.
\item
  \textbf{EM}: demands creativity in identifying the \textbf{missing
  data (complete data)} and technical skill in calculating an often
  complicated conditional expectation and then maximizing it
  analytically.
\item
  \textbf{MM}: requires creativity in identifying the surrogate
  function, using proper inequalities.
\end{itemize}
\end{frame}

\begin{frame}{Inequalities to construct majorizing/minorizing function}
\phantomsection\label{inequalities-to-construct-majorizingminorizing-function}
\begin{itemize}
\item
  \textbf{Property of convex function}: A function \(f: R^p \to R\) is
  \(\textbf{convex}\) if for all \(x_1, x_2 \in R^p\) and all
  \(\alpha \in [0,1]\), \[
   f(\alpha x_1 + (1-\alpha)x_2)\leq \alpha f(x_1) + (1-\alpha) f(x_2)
   \]
\item
  \textbf{Jensen's inequality}: for any convex function \(f\) and r.v.
  \(x\),
\end{itemize}

\[f[E(x)] \le E[f(x)]\]

\begin{itemize}
\tightlist
\item
  \textbf{Supporting hyperplanes}: If \(f\) is convex and
  differentiable, then
\end{itemize}

\[f(y)\geq f(x)+\nabla f(x)^\top(y-x), \forall x,y\in\mathbb{R}^p,\]

and equality holds when \(y = x\).

\note{\begin{itemize}
\tightlist
\item
  For concave function, jensen's inequality reverses
\item
  Draw supporting hyperplane from slide 7 of Hao MM lecture
\end{itemize}}
\end{frame}

\begin{frame}{Inequalities (continued)}
\phantomsection\label{inequalities-continued}
\begin{itemize}
\item
  \textbf{Arithmetic-Geometric Mean Inequality}: For nonnegative
  \(x_1,\ldots, x_m,\)
  \[\sqrt[m]{\prod_i^mx_i}\le \frac{1}{m}\sum_{i=1}^mx_i,\] and the
  equality holds iff \(x_1 = x_2 = \ldots = x_m\).
\item
  \textbf{Cauchy-Schwartz Inequality}: for \(p\)-vectors \(x\) and
  \(y\),
\end{itemize}

\[x^Ty\le ||x||\cdot ||y||,\]

where \(||x|| = \sqrt{\sum_i^p x_i^2}\) is the norm of the vector.

\note{Check that this definition of Cauchy-Schwartz is correct.
Shouldn't this be called the L2 norm? Or is it any norm?}
\end{frame}

\begin{frame}{Inequalities (continued)}
\phantomsection\label{inequalities-continued-1}
\begin{itemize}
\tightlist
\item
  \textbf{Quadratic upper bound}: If a convex function \(f(x)\) is twice
  differentiable and has bounded curvature, then we can majorize
  \(f(x)\) by a quadratic function with sufficiently high curvature and
  tangent to \(f(x)\) at \(x^t\). In algebraic terms, we can find a
  positive definite matrix \(M\) such that \(M-\nabla^2f(x)\) is
  nonnegative for all \(x\), then
\end{itemize}

\[f(x)\le f(x^t) + \nabla f(x^t)^T(x-x^t) + \frac{1}{2}(x-x^t)^TM(x-x^t)\]
provides a quadratic upper bound that majorizes \(f(x)\).

\note{How do you check if it has bounded curvature?}
\end{frame}

\begin{frame}{Example 1: EM algorithms}
\phantomsection\label{example-1-em-algorithms}
\begin{itemize}
\tightlist
\item
  By \textbf{Jensen's inequality} and the convexity of the function
  \(-\log(y)\), we have for probability densities \(a(y)\) and \(b(y)\)
  that
\end{itemize}

\[-\log \left\{E\left[\frac{a(y)}{b(y)}\right]\right\} \le E\left[ -\log\frac{a(y)}{b(y)} \right]\]

\begin{itemize}
\tightlist
\item
  \(Y\) has the density \(b(y)\), then \(E[a(y)/b(y)] = 1\). The left
  hand side vanishes, and we obtain \[E[\log a(y)] \le E[\log b(y)],\]
\end{itemize}

the Kullback-Leibler divergence.

\begin{itemize}
\tightlist
\item
  This inequality guarantees that a minorizing function is constructed
  in the E-step of any EM algorithm, making every EM algorithm an MM
  algorithm.
\end{itemize}

\note{I'm glossing over a lot of details here but this is basically
shorthand for another proof of EM that uses Jensen's inequality}
\end{frame}

\begin{frame}{Example 1: EM algorithms, cont}
\phantomsection\label{example-1-em-algorithms-cont}
\begin{itemize}
\tightlist
\item
  We have the \textbf{decomposition}
\end{itemize}

\begin{align}
Q(\theta|\theta^t) &= E_z\left[\log p(y, z |\theta)|y, \theta^t\right]\\[2mm]
&= E\left[\log p(z|y,\theta)|y, \theta^t\right] + \log p(y|\theta)
\end{align}

BY KL divergence,

\[E\left[\log p(z|y,\theta)|y, \theta^t\right] \le E\left[\log p(z|y,\theta^t)|y, \theta^t\right] \forall \theta\]

We obtain the \textbf{surrogate function} that minorizes the objective
function

\[ \log p(y|\theta) \ge Q(\theta|\theta^t) -  E\left[\log p(z|y,\theta^t)|y, \theta^t\right]\]
\end{frame}

\begin{frame}{Example 2: finding a sample median}
\phantomsection\label{example-2-finding-a-sample-median}
\begin{itemize}
\tightlist
\item
  Consider the sequence of numbers \(y_1, \ldots y_n\). The sample
  median \(\theta\) minimizes the \textbf{non-differentiable objective
  function}
\end{itemize}

\[f(\theta) = \sum_i^n |y_i-\theta|.\]

\begin{itemize}
\tightlist
\item
  The \textbf{quadratic function}
\end{itemize}

\[h_i(\theta|\theta^t) = \frac{(y_i-\theta)^2}{2|y_i-\theta^t|}+\frac 1 2 |y_i-\theta^t|\]
majorizes \(|y_i-\theta|\) at the point \(\theta^t\)
(Arithmetic-Geometric Mean Inequality).

\begin{itemize}
\tightlist
\item
  Hence, \(g(\theta|\theta^t) = \sum_i^n h_i(\theta|\theta^t)\)
  majorizes \(f(\theta)\).
\end{itemize}

\note{Can we draw a picture of this to make it more obvious?}
\end{frame}

\begin{frame}{Example 2: finding a sample median (continued)}
\phantomsection\label{example-2-finding-a-sample-median-continued}
We have the following objective function (a weighted sum of squares):

\[g(\theta|\theta^t) = \frac{1}{2}\sum_i^n\left[\frac{(y_i-\theta)^2}{|y_i-\theta^t|}+|y_i-\theta^t|\right]\]
- The \textbf{minimum} of \(g(\theta|\theta^t)\) occurs at

\[\theta^{t+1} = \frac{\sum_i^nw_i^ty_i}{w^t_i}, w_i^t = |y_i-\theta^t|^{-1}\]
- This algorithm works except when a weight \(w_i^t=\infty\). It
generalizes to sample quantiles, LASSO, and quantile regression.

\note{Might be good to think through how to generalize this to a
non-median quantile}
\end{frame}

\begin{frame}{Example 2: finding a sample median (continued)}
\phantomsection\label{example-2-finding-a-sample-median-continued-1}
Do lab exercise 1
\end{frame}

\begin{frame}{Finding a sample quantile}
\phantomsection\label{finding-a-sample-quantile}
Median example generalizes to finding a sample quantile. A \(q\)th
sample quantile of \(y_1,\ldots, y_n\) is one that minimizes the
function

\[f(\theta) = \sum_i p_q(y_i-\theta)\]

Where \(p_q(\theta) = q\theta\) if \(\theta \ge 0\) and
\(p_q(\theta) = -q(1-\theta)\) if \(\theta < 0\). A majorizing function
is

\[g_q(\theta|\theta^t) = \frac{1}{4}\sum_i^n\left[\frac{(y_i-\theta)^2}{|y_i-\theta^t|}+ (4q-2)(y_i-\theta)+ |y_i-\theta^t|\right]\]
\end{frame}

\begin{frame}{Finding a sample quantile}
\phantomsection\label{finding-a-sample-quantile-1}
\begin{center}
\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{mm.png}
\end{center}
\end{frame}

\begin{frame}{Example 3: Bradley-Terry Ranking}
\phantomsection\label{example-3-bradley-terry-ranking}
Consider a sports league with \(n\) teams. Assign team \(i\) the skill
level \(\theta_i\), where \(\theta_1 = 1\) for identifiability. Bradley
and Terry proposed the model

\[Pr(i \mbox{ beats } j) = \frac{\theta_i}{\theta_i + \theta_j}.\]

\begin{itemize}
\tightlist
\item
  If \(b_{ij}\) is the number of times \(i\) beats \(j\), then the
  likelihood of the data is
\end{itemize}

\[L(\theta) = \prod_{i\ne j}\left(\frac{\theta_i}{\theta_i + \theta_j}\right)^{b_{ij}}.\]
We estimate \(\theta\) by maximizing \(f(\theta) = \log L(\theta)\) and
then rank the teams on the basis of the estimates.
\end{frame}

\begin{frame}{Example 3: Bradley-Terry Ranking}
\phantomsection\label{example-3-bradley-terry-ranking-1}
\begin{itemize}
\item
  The log-likelihood is
  \(f(\theta) = \sum_{i \ne j}b_{ij}[\log\theta_i - \log(\theta_i + \theta_j)]\).
\item
  We need to \textbf{linearize} the term \(-\log(\theta_i + \theta_j)\)
  to \textbf{separate parameters}.
\end{itemize}

\note{This whole lecture is more about neat things you should come back
to if you need more rather than stuff to build on or try yourself,
necessarily}
\end{frame}

\begin{frame}{Example 3: Bradley-Terry Ranking (continued)}
\phantomsection\label{example-3-bradley-terry-ranking-continued}
\begin{itemize}
\tightlist
\item
  By the \textbf{supporting hyperplane property} when \(f\) is convex
  and the convecity of \(-\log(\cdot)\), we have
\end{itemize}

\[-\log(y)\ge -\log(x) - x^{-1}(y-x) = -\log(x) - y/x+ 1\]

\begin{itemize}
\tightlist
\item
  The inequality indicates that
\end{itemize}

\[-\log(\theta_i+\theta_j) \ge -\log(\theta_i^t+\theta_j^t) - \frac{\theta_i+\theta_j}{\theta_i^t+\theta_j^t}+ 1\]
\end{frame}

\begin{frame}{Example 3: Bradley-Terry Ranking (continued)}
\phantomsection\label{example-3-bradley-terry-ranking-continued-1}
\begin{itemize}
\tightlist
\item
  Thus, the \textbf{minorizing} function is
\end{itemize}

\[g(\theta|\theta^t) = \sum_{i\ne j}b_{ij}\left[ \log\theta_i -\log(\theta_i^t +\theta_j^t) - \frac{\theta_i + \theta_j}{\theta_i^t+\theta_j^t} + 1\right].\]

\begin{itemize}
\tightlist
\item
  The parameters are now \textbf{separated}. We can easily find the
  optimal point
\end{itemize}

\[\theta_i^t = \frac{\sum_{i\ne j}b_{ij}}{\sum_{i\ne j}(b_{ij}+b_{ji})/(\theta_i^t + \theta_j^t)}\]
\end{frame}

\begin{frame}{BT Ranking}
\phantomsection\label{bt-ranking}
Lab exercise 2
\end{frame}

\begin{frame}{Example 4: Handling constraints}
\phantomsection\label{example-4-handling-constraints}
\begin{itemize}
\item
  Consider the problem of \textbf{minimizing} \(f(\theta)\) subject to
  the \textbf{constraints} \(v_j(\theta)\ge 0\) for \(1\le j\le q\),
  where each \(v_j(\theta)\) is a concave, differentiable function.
\item
  By the \textbf{supporting hyperplane property} and the convexity of
  \(-v_j(\theta)\),
\end{itemize}

\[v_j(\theta^t) - v_j(\theta) \ge -[\nabla v_j(\theta^t)]^T(\theta-\theta^t) \tag{2}.\]

\begin{itemize}
\tightlist
\item
  Again, by the \textbf{supporting hyperplane property} and the
  convexity of \(-\log(\cdot)\), we have
  \(-\log y + log x \ge -x^{-1}(y-x) \implies x(-\log y + \log x) \ge x-y\).
  Then:
\end{itemize}

\begin{equation}
v_j(\theta^t)[-\log v_j(\theta)+\log v_j(\theta^t)]\ge v_j(\theta^t)-v_j(\theta).
\end{equation}\tag{3}
\end{frame}

\begin{frame}{Example 4: Handling constraints}
\phantomsection\label{example-4-handling-constraints-1}
By (2) and (3) on the previous slide,

\[v_j(\theta^t)[-\log v_j(\theta)+\log v_j(\theta^t)] + [\nabla v_j(\theta^t)]^T(\theta-\theta^t) \ge 0\]

and the equality holds when \(\theta = \theta^t\).

\begin{itemize}
\tightlist
\item
  Summing over \(j\) and multiplying by a positive tuning parameter
  \(\omega\), we construct the \textbf{surrogate function} that
  majorizes \(f(\theta)\),
\end{itemize}

\[g(\theta|\theta^t) = f(\theta) + \omega \sum_{j = 1}^q \left[v_j(\theta^t)\log\frac{v_j(\theta^t)}{v_j(\theta)} + [\nabla v_j(\theta^t)]^T(\theta-\theta^t) \right] \ge f(\theta)\]
\end{frame}

\begin{frame}{Handling constraints (continued)}
\phantomsection\label{handling-constraints-continued}
\begin{itemize}
\item
  Note:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Majorization gets rid of the inequality constraints}!
  \item
    The presence of \(\log v_j(\theta)\) ensures \(v_j(\theta)\ge 0\)
  \end{itemize}
\item
  An initial point \(\theta^0\) must be selected with all inequality
  constraints strictly satisfied. All iterates stay within the interior
  region but allows strict inequalities to become equalities at the
  limit
\item
  The minimization step of the MM algorithm can be carried out
  approximately by \textbf{Newton's method}.
\item
  Where there are linear equality constraints \(A\theta = b\) in
  addition to the inequality constraints \(v_j(\theta)\ge 0\), these
  should be enforced by introducing Lagrange multipliers during the
  minimization of \(g(\theta|\theta^t)\).
\end{itemize}

\note{We will talk more about constrained optimization and consider
different approaches in a later lecture}
\end{frame}

\begin{frame}{Comparing MM and Newton's Method}
\phantomsection\label{comparing-mm-and-newtons-method}
\begin{itemize}
\tightlist
\item
  \textbf{Convergence rate}

  \begin{itemize}
  \tightlist
  \item
    N: a quadratic rate
    \(\lim ||\theta^{t+1}-\hat{\theta}||/||\theta^{t+1}-\hat{\theta}||^2 = c\)
  \item
    MM: a linear rate
    \(\lim ||\theta^{t+1}-\hat{\theta}||/||\theta^{t+1}-\hat{\theta}|| = c < 1\)
  \end{itemize}
\item
  \textbf{Complexity of each iteration}

  \begin{itemize}
  \tightlist
  \item
    N: requires evaluation and inversion of Hessian, \(O(p^3)\)
  \item
    MM: separates parameters, \(O(p)\) or \(O(p^2)\)
  \end{itemize}
\item
  \textbf{Stability of the algorithm}

  \begin{itemize}
  \tightlist
  \item
    N: behaves poorly if started too far from an optimum point
  \item
    MM: guaranteed to increase/decrease the objective function at every
    iteration
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Comparing MM and Newton's Method}
\phantomsection\label{comparing-mm-and-newtons-method-1}
In conclusion, well-designed MM algorithms tend to require more
iterations but simpler iterations than Newton's method; thus MM
sometimes enjoy an advantage in computation speed and numerical
stability.
\end{frame}

\begin{frame}{Resources}
\phantomsection\label{resources}
\begin{itemize}
\tightlist
\item
  \href{https://www.stat.berkeley.edu/~aldous/Colloq/lange-talk.pdf}{Kenneth
  Lange lecture}
\item
  \href{https://hua-zhou.github.io/teaching/biostatm280-2017spring/slides/20-mm/mm.html}{Example
  with NMF}
\item
  \href{https://hua-zhou.github.io/teaching/biostatm280-2018spring/slides/20-mm/deLeeuw.pdf}{Lange
  examples of MM}
\end{itemize}

--
\end{frame}




\end{document}
