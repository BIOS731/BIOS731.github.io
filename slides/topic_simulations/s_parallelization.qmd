---
title: "Writing better, faster code in R"
author: "Julia Wrobel"
format:
  beamer:
    theme: metropolis
    aspectratio: 169
    fontsize: 14pt
    slide-level: 2
execute:
  echo: true
  eval: true
  warning: false
  message: false
header-includes:
  - \usepackage{xcolor}
  - \newcommand{\red}[1]{\textcolor{red}{#1}}
  - \newcommand{\green}[1]{\textcolor{green}{#1}}
---

## Overview

```{r, echo = FALSE}
library(tidyverse)
```

Today, we cover:

- Resampling methods
  - Permutation tests
  
- Improving code speed:
  - Benchmarking
  - Vectorization
  - Parallelization

**Announcements**:

- HW1 grades are up
- HW2 posted and due 2/11 at 10:00AM
- No class tomorrow (Thursday, January 29) but still have Office Hours




::: notes
Start homework 1!! It's computationally intensive. After this lecture you will have everything you need to start implementing it expect parallelization. You should have at least one simulation scenario tested and ready to go.

Question about homework 1?
Extra stuff in the Hesterburg paper that I didn't cover and is useful
:::

---

## Permutation tests

\fontsize{10pt}{11pt}\selectfont
Typically bootstrap is used for CI rather than hypothesis testing.  For hypothesis testing and p-values, we can use a **permutation test**.
- Idea: use resampling to generate a **null distribution** for a test statistic, then compare it to the one you observe in the real data

- **Null distribution**: the distribution of a quantity of interest (i.e. $\hat{\beta}$) if the null hypothesis $H_0$ is true

- The null distribution is available theoretically in some cases. For example,
assume $Y_i \sim N(\mu, \sigma^2), i = 1,\ldots,n$.  
  - Under $H_0: \mu = 0$, we have $\bar{Y} \sim N(0, \sigma^2/n)$
    - Test $H_0$ by comparing $\bar{Y}$ with $N(0, \sigma^2/n)$

- Use **permutation test** when null distribution cannot be obtained theoretically 



::: notes
This is a different resampling approach
We can use bootstrap for p-values but this isn't as common
:::

---


## Permutation tests

The basic procedure of permutation test for $H_0$:

- Permute data under $H_0$ $B$ times. Each time recompute the test
statistics. The test statistics obtained from the permuted data form the null distribution.
- Compare the observed test statistics with the null distribution to obtain statistical
significance.



---


## Permutation test example

\fontsize{10pt}{11pt}\selectfont
Assume there are two sets of independent normal r.v.’s with the same known
variance and different means:

- $X_i \sim N(\mu_1,\sigma^2)$
- $Y_i \sim N(\mu^2, \sigma^2)$

Our goal is to test $H_0: \mu_1 = \mu_2$.  Define test statistic: $t = \bar{X}-\bar{Y}$. Permutation test steps:

1. Randomly shuffle labels of $X$ and $Y$
2. Compute $t^* = \bar{X}^*-\bar{Y}^*$
3. Repeat `nperm` times. Resulting $t^*$ values form the **empirical null distribution** of $t$.
4. To compute p-values calculate $Pr(|t^*|> |t|)$



## Lab: resampling methods

We will spend the next 20-30 minutes going through this lab.

---

## Improving speed

Two ways to find bottlenecks in code:

- Benchmarking
- Profiling

---

## Comparing R codes for speed

\fontsize{11pt}{12pt}\selectfont
- R package `microbenchmark` is well-suited for comparing small chunks of code
- Your code can often be significantly improved


```{r}
library(microbenchmark)
x <- 120
microbenchmark(
  sqrt(x),
  x^(0.5)
)
```

::: notes
By default, 100 evaluations are run and the order of the two
expressions is randomized
:::


---

## Comparing R codes for speed

\fontsize{11pt}{12pt}\selectfont
```{r}
p <- 1000
x <- runif(p, min = 100, max = 120)
microbenchmark(
  sqrt(x),
  x^(0.5)
)
```

---


## Comparing R codes for speed

\fontsize{11pt}{12pt}\selectfont
```{r}
p <- 100000
x <- runif(p, min = 100, max = 120)
microbenchmark(
  sqrt(x),
  x^(0.5)
)
```


---

## Comparing R codes for speed

\fontsize{10pt}{11pt}\selectfont
- Take advantage of __crossprod__ and __tcrossprod__ functions. Suppose we want to calculate $x^{T}Ax$

```{r}
p <- 3000
x <- rnorm(p)
A <- matrix(rnorm(p^2), p, p)
microbenchmark(
    t(x) %*% A %*% x,
    crossprod(x, A %*% x)
)
```


::: notes
Certain linear algebra operations have faster or slower ways of writing them down. This time can add up if you do the operations many times or are working with a large dataset!
}
:::


---

## Example: R Loops

\fontsize{9pt}{10pt}\selectfont
R is very bad at resizing objects since it copies to resize

- Do not grow an object inside of a loop! 
- Instead, make an empty object first and then fill elements.

```{r}
bad = function (x){
  obj = c()
  for(i in 1:x){
    obj = c(obj, i)
    }
  return(obj)
}

better = function (x){
  obj = rep(NA, x)
  for (i in 1:x){
    obj[i] = i
    }
  return(obj)
}
```


---

## Example: R Loops

\fontsize{11pt}{12pt}\selectfont
```{r}
microbenchmark(bad (100), better (100))
```

---

## Measuring speed in simulations

\fontsize{10pt}{11pt}\selectfont
If you are interested in measuring computation time in a simulation study, say, to compare how fast different methods are, I **would not** recommend `microbenchmark`.  Instead, do the following:


```{r}
library(tictoc)

tic()
## do some stuff
large_vector <- rnorm(1e7)  # Create a vector of 10 million random numbers
sum_large_vector <- sum(large_vector)
time_stamp = toc(quiet = TRUE) # stop the timer and print the time elapsed

time_stamp$toc - time_stamp$tic # human time in seconds
```

---




## Vectorization


R is **vectorized**, meaning it efficiently performs operations on entire vectors or arrays in a single step, avoiding explicit loops and leveraging optimized low-level code for speed and simplicity.

- Often, there is more than one way to do something in R
- Take advantage of vectorization!
  - Often it is more concise and significantly faster

---

## Vectorization
  
\fontsize{11pt}{12pt}\selectfont
```{r}
# non vectorized squaring operation
x <- c(1, 2, 3, 4, 5)
result <- numeric(length(x))
for (i in seq_along(x)) {
  result[i] <- x[i]^2
}

# same operation, vectorized
x <- c(1, 2, 3, 4, 5)
result <- x^2
```

---

## Why vectorization?

\fontsize{11pt}{12pt}\selectfont
Another example- which is the vectorized version?


```{r}
x <- matrix(rnorm(30), 10, 3)


colMeans(x)
apply(x, 2, mean)
```



::: notes
Which is the vectorized version?
Base R has many built in functions that take advantage of this vectorization
:::

---


## Why vectorization?

\fontsize{11pt}{12pt}\selectfont
- __colSums__, __colMeans__ and corresponding row functions are vectorized

```{r}
n <- 100
p <- 3000
A <- matrix(rnorm(n * p), n, p)
microbenchmark(
    colMeans(A),
    apply(A, 2, mean)
)
```

---

## Break

Let's stop and try vectorization examples in the lab.


<br>
Next up will be parallelization.


---


## Parallel computing


\fontsize{12pt}{13pt}\selectfont
A modern CPU (Central Processing Unit) is at the heart of every computer. While traditional computers had a single CPU, modern computers can ship with multiple processors, which in turn can each contain multiple cores. These processors and cores are available to perform computations.

- A computer with one processor may still have 4 cores (quad-core), allowing 4 computations to be executed at the same time.
- **Parallel computing** is the simultaneous use of multiple processors or computers to solve a problem by dividing it into smaller, independent tasks, i.e. operating **in parallel**.


---


## Parallel computing

You can check how many **cores** your computer has to see how many tasks can be run at once.

```{r, eval = FALSE}
# Load the parallel package
library(parallel)

# Get the number of cores
detectCores()
# 12
```


::: notes
Some people say not to use all to cores at once because its good to leave one free for background stuff
:::

---

## When to parallelize

- Using $2$ cores does not mean your code will be $2\times$ faster
- Not all tasks can be parallelized


- Loops and repetitive tasks are great candidates
  - What are some computations we have looked at already that might be good candidates for parallelization?

::: notes
It’s not as simple as it may seem. While in theory each added processor would linearly increase the throughput of a computation, there is overhead that reduces that efficiency. For example, the code and, importantly, the data need to be copied to each additional CPU, and this takes time and bandwidth. Plus, new processes and/or threads need to be created by the operating system, which also takes time. This overhead reduces the efficiency enough that realistic performance gains are much less than theoretical, and usually do not scale linearly as a function of processing power. For example, if the time that a computation takes is short, then the overhead of setting up these additional resources may actually overwhelm any advantages of the additional processing power, and the computation could potentially take longer!
:::

---


## Example : foreach and doParallel


\fontsize{10pt}{11pt}\selectfont
```{r, echo = TRUE, eval = FALSE}
library(doParallel)
library(foreach)

# Set up parallel backend with 10 cores
num_cores = detectCores() - 2
cl = makeCluster(num_cores)
registerDoParallel(cl)

# define  Monte Carlo function to estimate Pi
monte_carlo_pi <- function(n) {
  inside_circle <- sum(runif(n, -1, 1)^2 + runif(n, -1, 1)^2 <= 1)
  return((inside_circle / n) * 4)
}

nsim = 100

```

::: notes
Note that this is stored in a vector. You may need other options for storage-what if you want to store it as a list?
combine argument- default is to return a list, this concatenates things into a vector. You could also supply your own function here
:::

---


### Example : foreach and doParallel


\fontsize{12pt}{13pt}\selectfont
```{r, echo = TRUE, eval = FALSE}
pi_estimates = foreach(i = 1:nsim, .combine = c) %dopar% {
  n = 1000
  monte_carlo_pi(n)
}

# overall estimate of pi
mean(pi_estimates)
```

::: notes
Note that this is stored in a vector. You may need other options for storage-what if you want to store it as a list?
combine argument- default is to return a list, this concatenates things into a vector. You could also supply your own function here
:::

---

## Example : foreach and doParallel

\fontsize{10pt}{11pt}\selectfont
Useful arguments include multiple iterators, error catching, combine
  
```{r, eval = FALSE}
foreach(
  ...,
  .combine,
  .init,
  .final = NULL,
  .inorder = TRUE,
  .multicombine = FALSE,
  .maxcombine = if (.multicombine) 100 else 2,
  .errorhandling = c("stop", "remove", "pass"),
  .packages = NULL,
  .export = NULL,
  .noexport = NULL,
  .verbose = FALSE
)

```



---


## Resources

- [Advanced R: chapters 22-24](https://adv-r.hadley.nz/perf-improve.html)
- [foreach vignette](https://cran.r-project.org/web/packages/foreach/vignettes/foreach.html)
- [furrr package for tidyverse parallelization](https://furrr.futureverse.org/)

---

## Extra
  
---

## Profiling your code

Some good references on profiling to learn more (a lot of overlap):

 * [Advanced R](https://adv-r.hadley.nz/perf-measure.html)
 
 * [profvis R package](https://rstudio.github.io/profvis/)
 
 * [Rstudio guide](https://support.rstudio.com/hc/en-us/articles/218221837-Profiling-with-RStudio)
 
 * [proftools R package](https://cran.r-project.org/web/packages/proftools/vignettes/proftools.pdf)

There has been some changes on how profiling works from R v.3 to v.4 which (sometimes) makes profiler output confusing

---



## Profiling in R


- `profvis` is built on R's built-in profiler tool, `Rprof`
- `Rprof` is a statistical profiler that uses sampling
- When you run `profvis`, it stops the R interpreter every 10ms
(default interval) and records which function is currently
executing, as well as the entire call stack (i.e., which function
called that function)
  - The results are not deterministic 

---

## Profiling in R


From [R programming for Data Science](https://bookdown.org/rdpeng/rprogdatascience/profiling-r-code.html)

> "Rprof() keeps track of the function call stack at regularly sampled intervals and tabulates how much time is spent inside each function. By default, the profiler samples the function call stack every 0.02 seconds. This means that if your code runs very quickly (say, under 0.02 seconds), the profiler is not useful. But if your code runs that fast, you probably don’t need the profiler."


---

## Profiling: return to Ex. 2 (powers of a matrix)

**Rprof()** function gives a report of (approximately) how much time each function/operation within your code takes. To see the effect of memory allocation, enable tracking of **garbage collection** (GC)


```{r, echo = T, cache = T, eval = F}
Rprof(gc.profiling = TRUE) # start monitoring
invisible(powers1(x, 8)) # suppress function output
Rprof(NULL) # stop monitoring
summaryRprof() # see the report
```

---

## `Rprof()`

* **by.total** divides the time spend in each function by the total run time
* **by.self** first subtracts out time spent in functions above the current function in the call stack (**more useful typically**)


---

## Garbage collection (GC)

**Garbage collection** - freeing the memory from objects that are no longer in use
[(more in Section 2.6 of Advanced R)](https://adv-r.hadley.nz/names-values.html#gc)

If significant time of your program is spend in GC

  * you may be doing a lot of dynamic memory allocation (the case of powers1)
  
  * you may be storing a lot of temporary objects
  
  * you may be consistently changing objects of large sizes

---

## Profiling: return to Ex. 3 (powers of a matrix)

```{r}
powers3 <- function(x, dg){
  # allocate memory in advance
  pw <- matrix(x, length(x), dg) 
  prod <- x # current product
  for (i in 2:dg){
    prod <- prod * x
    pw[ , i] <- prod # no cbind
  }
  return(pw)
}

```



