---
title: "Resampling methods: Bootstrap and Permutation tests"
author: "Julia Wrobel"
format:
  beamer:
    theme: metropolis
    aspectratio: 169
    fontsize: 14pt
    slide-level: 2
execute:
  echo: true
  eval: true
  warning: false
  message: false
header-includes:
  - \usepackage{xcolor}
  - \newcommand{\red}[1]{\textcolor{red}{#1}}
  - \newcommand{\green}[1]{\textcolor{green}{#1}}
---

## Overview


Today, we cover:

- Resampling methods
  - Bootstrap
  - Permutation tests

**Announcements**:

- HW2 posted and due 2/4 at 10:00AM

Readings:

- [Tim Hesterburg: Bootstrap](https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.182)


::: notes
Start homework 1!! It's computationally intensive. After this lecture you will have everything you need to start implementing it expect parallelization. You should have at least one simulation scenario tested and ready to go.

Question about homework 1?
Extra stuff in the Hesterburg paper that I didn't cover and is useful
:::

---

## Bootstrap: motivation

What do you already know about the bootstrap?

::: notes
What is your prior experience with the bootstrap?  What do you know about it already?

better confidence intervals when we can't get or don't trust ones based on CLT/ normal sampling theory

Confidence intervals are for parameter estimates (like $\hat{\beta}$)
:::

---

## Bootstrap: motivation

- Basically, used to obtain better confidence intervals when we can't get or don't trust ones based on CLT
  - idea: $\hat{\beta} = (x^Tx)^{-1}x^Ty$, $se(\hat{\beta}) = \sigma^2(x^Tx)^{-1}$
    - What happens if we don't trust $se(\hat{\beta})$


- Resample data with replacement, calculate a statistic of interest (e.g., $\bar{Y}$) for each sample, obtain empirical distribution of $\bar{Y}$
  - Nonparametric bootstrap


How would you then obtain confidence intervals?

::: notes
CI: we will learn 4 ways to do this
Think about how you construct T statistic or Z-statistic.  CI and p-values are based on assumption that the statistic should be roughly $t$ or $N(0,1)$ distributed. If this is not true, the CI will not be good.  
:::


---



## Bootstrap: motivation

- Many subtle issues with the usual non-parametric bootstrap
  - In small samples, bootstrap confidence intervals often preferred to avoid relying on CLT, but the common percentile bootstrap
performs poorly in small samples!
  - Other types of bootstrap intervals may work better in small samples.


- Important to understand these issues of the bootstrap to avoid pitfalls
when quantifying uncertainty



---



## Notation


For some cumulative distribution function $F$, suppose we are
interested in a parameter, $\theta \stackrel{\triangle}{=} T(F)$, written as a functional of $F$


- Let $y$ represent a random variable associated with a sample from $F$
- A **functional** is a function of a function
  - Mean of $F$: $T(F) = \int y dF(y)$
  - Median of $F$: $T(F) = inf\{y: F(y) \ge 0.5\}$



::: notes
Here  T(F) means some functional of f.  

inf is the infimum, which is the biggest number that is less than or equal to all elements of its set
This is our mathematical definition of the median in terms of the CDF

The idea here is that both the mean and the median (or other summary statistics we might be interested in) depend on the true underlying CDF of the data.
:::

---


## Notation 

\fontsize{10pt}{11pt}\selectfont
- Usually $F$ is unknown, so we have to estimate it. 
- Suppose we have a sample of data, $\mathcal{Y} = \{ y_1, y_2, \ldots, y_n\}$ drawn from $F$

- Empirical CDF is an estimate of $F$
  - places $1/n$ mass at each observed data point:

$$\hat{F}_n(y) = \frac{1}{n}\sum_{i = 1}^n I_{y_i\le y}$$

- To estimate a functional $T(F)$, we plug in the empirical distribution, $\hat{F}_n(y)$, for $F(y)$:

$$T(\hat F_n) = \int g(y) d\hat F_n(y) = \frac 1n\sum_{i = 1}^n g(y_i)$$

::: notes
Idea here is that, even though we don't usually think of the mean or other statistics of interest as being dependent on the empirical CDF, conceptually they are.

The notation here gets at the underlying theory of the bootstrap, which we won't go into too much and will just touch the surface of here.

You test statistic of interest can be posed as a function of the empirical CDF
:::

---

## Notation

- Now, suppose we are interested in statistical **inference** for a statistic
  - Denote this $W(\mathcal{Y, F})$

- For example, if $Y_i \sim N(\mu, \sigma^2)$ we might be interested in the t-statistic

$$W(\mathcal{Y, F}) = t = \frac{\bar{Y}-\mu}{s/\sqrt{n}}$$

::: notes
Inference for $W(\mathcal{Y, F})$ depends on the sampling distribution of W
:::

---


## Notation

\fontsize{10pt}{11pt}\selectfont
- Now, suppose we are interested in statistical **inference** for a statistic
  - Denote this $W(\mathcal{Y, F})$

- For example, if $Y_i \sim N(\mu, \sigma^2)$ we might be interested in the t-statistic

$$W(\mathcal{Y, F}) = t = \frac{\bar{Y}-\mu}{s/\sqrt{n}}$$

- In many cases, the sampling distribution of $W(\mathcal{Y, F})$ may be very complicated, unknown, or could depend on an unknown $F$
  - e.g., if $Y_i$ come from a skewed distribution, then $t$ does not have a t-distribution, **even in large samples**!

::: notes
Why is this last statement a problem?
:::

---


## When t is not t 

\fontsize{10pt}{11pt}\selectfont
When $Y_i$ come from a skewed distribution, the sample mean and its SE are correlated and their correlation does not decrease as $n\to \infty$

- $\implies$ t-statistic is not t-distributed

![](tdist.png){width=60%}



- If your test or CI assumes your statistic has a normal or t sampling distribution, **any deviation** from the diagonal implies your test/interval will perform poorly


::: notes
This is a Q-Q plot for a **sampling distribution** not raw data! The CLT already had its chance to work
:::

---


## When t is not t 

Idea: confidence intervals for parameter estimates provide no robustness against non-normality!
- Coverage does not improve as $n \to \infty$!

\vspace{5mm}
Bootstrapping can be very useful for constructing better confidence
intervals for these parameters.


::: notes
Tricky here because the non-normality refers to the sampling distribution of your test-statistic, not the underlying distribution of your data
:::

---

## The Bootstrap Principle

- Replace unknown $F$ with $\hat F_n$
  - Produce a numerical approximation of the sampling distribution of $W(\mathcal{Y, F})$ by repeatedly sampling from $\hat F_n$



**Idea**: Bootstrap principle is used to estimate the sampling distribution of a *statistic* without relying on strong parametric assumptions about the underlying population distribution

::: notes
Used to estimate the sampling distribution of a statistic without relying on strong parametric assumptions about the underlying population distribution
:::

---



## The Bootstrap Principle

- Nonparametric bootstrap
- Parametric bootstrap
- Wild bootstrap


- Smooth bootstrap
- Bag of little bootstraps

::: notes
Many different flavors!
:::

---


## Nonparameteric bootstrap

\fontsize{10pt}{11pt}\selectfont
A **bootstrap sample** is a random sample of size drawn **with replacement** from $\hat F_n$

- bootstrap sample denoted $y^* = (y^*_1, y^*_2, \ldots, y^*_n)$
- $\hat{\theta}^* \stackrel{\triangle}{=} T(\hat F_n(y^*))$ is the corresponding bootstrap estimate using the sample $y^*$
  - We'll call this $T(\hat F_n^*)$ to simplify notation
  

Draw $B$ bootstrap samples and calculate $\hat{\theta}^*_i, i = 1,\ldots,B$. 

- Then, SE of $\hat{\theta}$ can be estimated using the standard deviation of the $B$ replications:

$$\widehat{SE}(\hat{\theta}) = \sqrt{\frac{\sum_{i = 1}^B (\hat{\theta_i^*}-\bar{\theta}^*)^2}{B-1}}$$


::: notes
Then use this standard error or other aspects of the distribution of theta hat to estimate a confidence interval
:::

---


## Why does it work?

- $\hat F_n \to F$ uniformly by Glivenko-Cantelli

  - $\implies$ when $n$ is large, iid sampling from $\hat F_n$ should be approximately the same as iid sampling from $F$
  
- Each bootstrap sample is considered an iid sample from $\hat F_n$
  

- Potential sources of error:
  - Original sample is chosen randomly from population
  - Bootstrap distribution contains Monte Carlo error when exhaustive resampling is infeasible
    - For fixed $n$, number of possible bootstrap samples is ${2n-1}\choose{n}$


::: notes
Data sample needs to be representative of the population for it to work
Glivenko Cantelli establishes uniform convergence of the eCDF to the true CDF

Idea behind the bootstrap is simple but the theory is hard! I can point you to some resources if you want more.

if n = 10, choose(2n-1, n) = 92,378

if n = 5, 126
:::

---



## Sources of error


![](booterror.png){width=60%}

::: notes
One takeaway from this plot: bootstrap distribution is centered around $\bar{x}$, not $\mu$.  **Bootstrapping will not get you a better estimate of ** $\bar{x}$! Just a better estimate of the spread around it.

Also, there is error both due to your original sample, and due to the randomness in your bootstrap samples. The larger nboot is, the more you will reduce the second source of error. You can't really do anything about the first.
:::

---

## Example: nonparameteric bootstrap

\fontsize{10pt}{11pt}\selectfont
Using `mtcars` dataset, we will look at the mean `mpg` for different cars

```{r}
data(mtcars)
str(mtcars)
```


---

## Example: nonparameteric bootstrap


```{r, echo = FALSE, fig.width= 6, fig.height = 4, fig.align='center'}
hist(mtcars$mpg)
```



---

## Example: nonparameteric bootstrap

\fontsize{10pt}{11pt}\selectfont
```{r}
B = 10000
mpg = mtcars$mpg
boot_mean = rep(NA, B)
set.seed(222)
for(i in 1:B){
  ystar = sample(mpg, size = length(mpg), replace = TRUE)
  boot_mean[i] = mean(ystar)
}

paste0("mean: ", round(mean(mpg),3), 
       "; bootstrapped mean: ", round(mean(boot_mean),3))
```


::: notes
Formula for standard error of the mean is s/sqrt(n) where s is the sample standard deviation
depends on large enough sample size and/or normal mpg variable

Standard deviation of the bootstrapped estimates is an estimate of the standard error of theta hat
:::

---

## Example: nonparameteric bootstrap

\fontsize{10pt}{11pt}\selectfont

```{r}
se_mean = sd(mpg)/sqrt(length(mpg))
paste0("sd: ", round(se_mean,3), 
       "; bootstrapped sd: ", round(sd(boot_mean),3))
```

---


## Example: nonparameteric bootstrap

```{r, fig.width= 5, fig.height = 3, fig.align='center'}
hist(boot_mean)
```


---





## Bootstrap confidence intervals

- When it works, the bootstrap can be very useful for constructing
confidence intervals (CIs) for a parameter $\theta$


- Multiple methods for constructing bootstrap CIs exist!
  - Percentile intervals
  - $t$ intervals with bootstrap SE (also called Wald-type)
  - bootstrap $t$
  - BCa intervals
  

- Quality of different approaches generally depends on $n$ and the skewness of the original data


---

## Order of accuracy

- Recall "big-O" notation for non-negative functions $f,g$:
  - $f(x) \in O(g(x)) \mbox{ iff } f(x) \le hg(x) \forall x\ge x_0$ and some $h > 0$
  

- A confidence interval is **first-order accurate** if the non-coverage probability differs from the nominal value by $O(n^{-1/2})$:
  - $Pr(\theta < \theta_{lb}) + P(\theta > \theta_{ub}) = \alpha + O(n^{-1/2})$
  
  
- A confidence interval is second-order accurate if the non-coverage probability differs from the nominal value by $O(n^{-1})$:
  - $Pr(\theta < \theta_{lb}) + P(\theta > \theta_{ub}) = \alpha + O(n^{-1})$

::: notes
Basically, this just means that the second order accurate confidence intervals converge faster as n increases than the second order intervals.
:::

---


## Percentile method

The simplest approach to constructing a bootstrap CI is the **percentile method**.

- Let $\hat{\theta}^*_1, \ldots, \hat{\theta}^*_B$ be a bootstrap sample of point estimators
- Then, a two-sided $100 \times (1-\alpha)$% bootstrap percentile CI is 

$$(\xi^*_{\alpha/2}, \xi^*_{1-\alpha/2})$$

- where $\xi^*_p$ is the $p^{th}$ percentile of the bootstrap samples


---

## Percentile method

**Pros**:

- Simple to implement and easy to understand
- It is **transformation invariant**: the percentile method confidence interval for a monotone transformation of $\theta$ can be obtained by transforming the endpoints of the interval for $\theta$
- Works better than some other methods when data are skewed


**Cons**:

- Tends to be too narrow with small sample sizes
- The percentile method is only first-order accurate


::: notes
Think about transformation invariant in terms of logistic regression: you might want a confidence interval for both beta and e^beta (the odds ratio)

What does too narrow mean, practically? Poor coverage. False rejection of null hypothesis (type 1 error)
::: 

---

## $t$ intervals with bootstrap SE

Recall that if $(\hat{\theta}-\theta)/\hat{se}(\hat{\theta}) \to N(0,1)$, then the (Wald) approximate CI for $\theta$ is 

$$\hat{\theta} \pm Z_{1-\alpha/2}\times se(\hat{\theta)}$$

- This interval will closely agree with the percentile bootstrap CI when $\hat{\theta}$ is approximately normal
- What if we don't know $se(\hat{\theta)}$ or $se(\hat{\theta)}$ is difficult to calculate?

---

## $t$ intervals with bootstrap SE

When $se(\hat{\theta)}$ is unknown or cumbersome, we can use a bootstrap estimate of the standard error of $\hat{\theta}$ instead:

$$\hat{\theta} \pm Z_{1-\alpha/2}\times \hat{se}_b(\hat{\theta)}$$

- For smaller samples, replace the $Z$ quantile by a $t$ quantile:

$$\hat{\theta} \pm t_{1-\alpha/2, n-1}\times \hat{se}_b(\hat{\theta})$$

- If you can estimate $se(\hat{\theta)}$ directly, there is no real benefit of using $\hat{se}_b(\hat{\theta})$

---

## $t$ intervals with bootstrap SE

**Pros**:

- Simple to implement and easy to understand
- Can be applied to situations where $se(\hat{\theta)}$ is difficult to derive


**Cons**:

- **Biased**: comparable to using the MLE $\sqrt{\frac{1}{n}\sum_i (x_i-\bar{x})^2}$ instead of the unbiased estimate $\sqrt{\frac{1}{n-1}\sum_i (x_i-\bar{x})^2}$
- Can perform poorly if distribution is skewed
- Tends to be too narrow with small sample sizes
- Only first-order accurate


---

## Bootstrap $t$ method

The **bootstrap-t** method uses simulation to estimate the $t$ quantile for the interval.

- For **each** bootstrap sample, calculate $t^*_b$, where:
  - $t^*_b = \frac{\hat{\theta}^*_b-\hat{\theta}}{se(\hat{\theta}^*_b)}$
  - $\hat{\theta}$ is estimated from the original sample
  - $\hat{\theta}^*_b$ is estimated from the $b^{th}$ bootstrap sample
  - $se(\hat{\theta}^*_b)$ is the SE of the $b^{th}$ estimate
  
::: notes
This is different from the previous approach, t intervals with bootstrap SE!!
Notation here is key to understand, this one is going to show up in your homework
:::

---

## Bootstrap $t$ method

The **bootstrap-t** method uses simulation to estimate the $t$ quantile for the interval.

- For **each** bootstrap sample, calculate $t^*_b$, where:
  - $t^*_b = \frac{\hat{\theta}^*_b-\hat{\theta}}{se(\hat{\theta}^*_b)}$
  - $\hat{\theta}$ is estimated from the original sample
  - $\hat{\theta}^*_b$ is estimated from the $b^{th}$ bootstrap sample
  - $se(\hat{\theta}^*_b)$ is the SE of the $b^{th}$ estimate
  

- Since $se(\hat{\theta}^*_b)$ is unknown, must estimate it...

---

## Bootstrap $t$ method

\fontsize{10pt}{11pt}\selectfont
Note that calculating $\hat{se}(\hat{\theta}^*_b)$ for each bootstrap estimate requires (nested) bootstrapping!

- Obtain a bootstrap estimate of $se(\hat{\theta}^*_b)$ for **each** $b = 1, \ldots, B$
  - For each bootstrap sample $b$, run $k = 1, \ldots, K$ bootstrap simulations to obtain $\hat{\theta}_{b,k}^*$ and use these to estimate  $se(\hat{\theta}^*_b)$ for calculating $t^*_b$
- Calculate lower and upper quantiles of the *bootstrap t distribution*, $t^*_{1-\alpha/2}$ and $t^*\alpha/2$ and construct the CI:

$$\left(\hat{\theta}-t^*_{1-\alpha/2}\times se(\hat{\theta)}, \hat{\theta}-t^*_{\alpha/2}\times se(\hat{\theta)}\right)$$

- $se(\hat{\theta})$ is the SE of the estimate from the original sample, or can be estimated using the top-level bootstrap



---


## Bootstrap $t$ method

\fontsize{10pt}{11pt}\selectfont
**Pros**:

- Second-order accurate
- Usually outperforms the other methods discussed so far, especially for non-normal populations.
- For skewed data, much more asymmetric than the percentile bootstrap interval which corrects for the possibility that we observed too few observations from the tail


**Cons**:

- Requires an iterated bootstrap
- The bootstrap-t interval is not transformation invariant. Must reconstruct the CI for a function of the parameter $g(\theta)$


---



## BCa Bootstrap

\fontsize{10pt}{11pt}\selectfont
The bias-corrected accelerated (BCa) method is an improvement over the percentile method, though still does not work well in small sample sizes.

- BCa intervals use percentiles of the bootstrap distribution, but not necessarily the $100 \times \alpha$-th and $100\times (1-\alpha)$-th percentiles
- Adjusts for **bias** in the bootstrap distribution and variability in the precision of the estimate (aka **acceleration**)

First, estimate a bias correction factor $\hat{z}_0$:

$$\hat{z}_0 = \Phi^{-1}\left(\frac{\#\{\hat{\theta}^*<\hat{\theta}\}}{B}\right)$$


::: notes
Bias correction. based on the proportion of bootstrap sample estimates that are less than the original sample estimate.
:::

---

## BCa Bootstrap

Next, estimate the acceleration parameter $\hat{a}$:

$$\hat{a} = \frac{\sum_{i=1}^n(\hat{\theta}_{(.)}-\hat{\theta}_{(i)})^3}{6\left(\sum_{i=1}^n (\hat{\theta}_{(.)}-\hat{\theta}_{(i)})^2\right)^{3/2}}$$

- $\hat{\theta}_{(i)}$ is the value of the statistic with the $i^{th}$ observation removed (i.e. the $i^{th}$ jackknife estimate)
- $\hat{\theta}_{(.)}$ is the mean of the $n$ jackknife estimates

- adjusts for the rate of change in the standard error as the estimate changes


---

## BCa Bootstrap

\fontsize{10pt}{11pt}\selectfont
After estimating the bias correction $\hat{z}_0$ and acceleration $\hat{a}$, compute:

$$\alpha_1 = \Phi\left(\hat{z}_0 + \frac{\hat{z}_0 + z_{\alpha/2}}{1-\hat{a}(\hat{z}_0 + z_{\alpha/2})}\right)$$


$$\alpha_2 = \Phi\left(\hat{z}_0 + \frac{\hat{z}_0 + z_{1-\alpha/2}}{1-\hat{a}(\hat{z}_0 + z_{1-\alpha/2})}\right)$$

- $\Phi$ is the standard normal CDF (`pnorm`)
- $z_a$ is the $100\times \alpha$-th percentile of the standard normal
- Use these new percentiles to construct a CI
  - The BCa interval is the same as the percentile interval when $\hat{a} = \hat{z}_0 = 0$

---


## BCa Bootstrap

\fontsize{10pt}{11pt}\selectfont
**Pros**:

- Second-order accurate and transformation invariant
- Works well for a variety parameters
- For skewed data, the percentile bootstrap is not asymmetric
enough and the BCa bootstrap addresses this limitation
- Implemented in the `bcanon()` function in the `bootstrap` R library



**Cons**:

- Estimation of the acceleration parameter requires the jackknife
- Less intuitive than other methods
- Still doesn't work great for small sample sizes

::: notes
Considered the gold standard for nonparametric bootstrap methods
:::

---



## Parametric bootstrap


If we have information about the population distribution, this can be
used in resampling for bootstrap inference

- If the assumption about the population distribution is correct then
the parametric bootstrap will perform better than the
nonparametric bootstrap

- If the assumption not correct, then the nonparametric bootstrap should perform better.

---

## Parametric bootstrap example

\fontsize{10pt}{11pt}\selectfont
Suppose we have data from a $N(\mu, \sigma^2)$ distribution, $Y = \{y_1, \ldots, y_n\}$

- Interested in obtaining an estimate of the standard error of the
trimmed mean with 10% trimmed from each tail
- To employ the parametric bootstrap, we would start by generating $n$ values from a $N(\hat{\mu}, \hat{\sigma}^2)$
  - $\hat{\mu}, \hat{\sigma}^2$ are the ML estimates
- Then, compute trimmed mean using the simulated sample
  - repeat B times



Only difference between parametric and non-parametric bootstrap is the way to generate data!


::: notes
Note that no replacement is done here

In parametric bootstrap: simulate from parametric distribution
In non-parametric: sample with replacement from observed data
Still can use same means of getting confidence intervals
:::

---

## Bootstrapping regression models

Regression model:

$$E(Y_i|\textbf{X}_i) = f(\mathbf{\beta}, \textbf{X}_i), i = 1,\ldots, n$$



- $f$ is a known function, $\textbf{X}_i$ a vector of parameters
- Interested in estimating a SE or CI for a function of parameters


::: notes
Generalizes to logistic regression, etc
:::

---


## Bootstrapping regression models

Three common methods:

1. Bootstrap the pairs $(Y_i, \textbf{X}_i)$ and generate bootstrap
parameter estimates that can be used to obtain bootstrap CIs

2. Bootstrap the **residuals** of the original model

3. Estimate the residual variance and simulate residuals


::: notes
Are these parametric or nonparametric bootstrap approaches?
:::

---

## Bootstrapping regression models: residuals 

\fontsize{10pt}{11pt}\selectfont
Let's take linear regression as an example, 

$$Y_i = \textbf{X}_i^T\mathbf{\beta} + \epsilon_i; \epsilon_i \sim N(0, \sigma^2)$$

To fit a bootstrap on the residuals:

- Fit a model on the original data to obtain residuals $\hat{\epsilon}_i$
  - $\hat{\epsilon}_i = Y_i - \textbf{X}_i^T\hat{\mathbf{\beta}}$
- Resample residuals $\hat{\epsilon}_i$ with replacement and repeat $B$ times:
  - Compute $Y_i^b = \textbf{X}_i^T\hat{\mathbf{\beta}}+ \hat{\epsilon}_i^b$ for all $i$
  - Refit model using $Y_i^b, i = 1,\ldots, n$ to estimate $\hat{\beta}^b$
- This approach assumes errors are **identically distributed**



::: notes
This is the approach where we bootstrap the residuals of the original model
Advantage: Y's and X's are fixed rather than random, which is sometimes consistent with assumptions
:::

---


## Bootstrapping regression models: residual variance 

- Estimate residual variance from sample, $\hat{\sigma}^2$
- Repeat $B$ times:
  - Generate a residual $\epsilon^b_i \sim N(0, \hat{\sigma}^2)$ and associated outcome $Y_i^b = \textbf{X}_i^T\hat{\mathbf{\beta}} + \epsilon^b_i$ for $i = 1,\ldots, n$
  - Refit model using $Y_i^b$, $i = 1, \ldots, n$ to estimate $\hat{\beta}^b$


This approach assumes the residual errors are **normally distributed** 

::: notes
This is the third approach, where we estimate residual variance
:::

---




## Wild bootstrap

\fontsize{10pt}{11pt}\selectfont
$$Y_i = \textbf{X}_i^T\mathbf{\beta} + \epsilon_i$$
The models previously discussed fail if there is **heteroskedasticity**
- i.e., if $\epsilon_i$ are not identically distributed

The **wild bootstrap** is a modification of the residual bootstrap intended to handle heteroskedasticity.

- Fit a model on the original data to obtain residuals $\hat{\epsilon}_i$
- Repeat $B$ times:
  - Multiply residuals by random weights to obtain $\hat{\epsilon}_i^b$
  - Compute $Y_i^b = \textbf{X}_i^T\hat{\mathbf{\beta}}+ \hat{\epsilon}_i^b$ for all $i$
  - Refit model using $Y_i^b, i = 1,\ldots, n$ to estimate $\hat{\beta}^b$



---

## Wild bootstrap

Common options for weights:


1. Multiply $\hat{\epsilon}_i$ by $-1$ or $1$ with equal probability


2. Multiply $\hat{\epsilon}_i$ by a standard normal random variable

---



## When does the bootstrap fail?

We must assume that the original sample is representative of the population so that $\hat{F}_n$ is a good estimate of $F$


The bootstrap may fail when:

- The support of $F$ depends on the parameter of interest
- The true parameter sits on the boundary of the parameter space
- The parameter of interest is nonregular, i.e., there does not exist
an estimator that converges to it uniformly in distribution over
the parameter space


---

## Permutation tests

\fontsize{10pt}{11pt}\selectfont
Typically bootstrap is used for CI rather than hypothesis testing.  For hypothesis testing and p-values, we can use a **permutation test**.
- Idea: use resampling to generate a **null distribution** for a test statistic, then compare it to the one you observe in the real data

- **Null distribution**: the distribution of a quantity of interest (i.e. $\hat{\beta}$) if the null hypothesis $H_0$ is true

- The null distribution is available theoretically in some cases. For example,
assume $Y_i \sim N(\mu, \sigma^2), i = 1,\ldots,n$.  
  - Under $H_0: \mu = 0$, we have $\bar{Y} \sim N(0, \sigma^2/n)$
    - Test $H_0$ by comparing $\bar{Y}$ with $N(0, \sigma^2/n)$

- Use **permutation test** when null distribution cannot be obtained theoretically 



::: notes
This is a different resampling approach
We can use bootstrap for p-values but this isn't as common
:::

---


## Permutation tests

The basic procedure of permutation test for $H_0$:

- Permute data under $H_0$ $B$ times. Each time recompute the test
statistics. The test statistics obtained from the permuted data form the null distribution.
- Compare the observed test statistics with the null distribution to obtain statistical
significance.



---


## Permutation test example

\fontsize{10pt}{11pt}\selectfont
Assume there are two sets of independent normal r.v.â€™s with the same known
variance and different means:

- $X_i \sim N(\mu_1,\sigma^2)$
- $Y_i \sim N(\mu^2, \sigma^2)$

Our goal is to test $H_0: \mu_1 = \mu_2$.  Define test statistic: $t = \bar{X}-\bar{Y}$. Permutation test steps:

1. Randomly shuffle labels of $X$ and $Y$
2. Compute $t^* = \bar{X}^*-\bar{Y}^*$
3. Repeat `nperm` times. Resulting $t^*$ values form the **empirical null distribution** of $t$.
4. To compute p-values calculate $Pr(|t^*|> |t|)$





