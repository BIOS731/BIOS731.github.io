[
  {
    "objectID": "topic_bayesian_computing.html",
    "href": "topic_bayesian_computing.html",
    "title": "Bayesian Computing",
    "section": "",
    "text": "In progress."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "BIOS 731: Advanced Statistical Computing",
    "section": "",
    "text": "Department: Biostatistics and Bioinformatics\nCourse Number: BIOS 731\nSection Number: 1\nCredit Hours: 4\nSemester: Spring 2026\nClass Hours and Location:\nWednesday & Thursday, 10:00 AM – 11:50 AM, GCR P39\nInstructor:\nJulia Wrobel, PhD\nEmail:\njulia.wrobel@emory.edu\nSchool Address / Mailbox:\n1518 Clifton Rd, 1518-002-3AA\nOffice Hours:\nThursdays 1:00 PM- 2:00 PM\nGCR Room 210\nTeaching Assistant:\nWeijia Qian (weijia.qian@emory.edu)\nTA Office Hours: Tuesdays 11:00 AM - 12:00 PM GCR Room 369"
  },
  {
    "objectID": "syllabus.html#instructor-contact-information",
    "href": "syllabus.html#instructor-contact-information",
    "title": "BIOS 731: Advanced Statistical Computing",
    "section": "",
    "text": "Department: Biostatistics and Bioinformatics\nCourse Number: BIOS 731\nSection Number: 1\nCredit Hours: 4\nSemester: Spring 2026\nClass Hours and Location:\nWednesday & Thursday, 10:00 AM – 11:50 AM, GCR P39\nInstructor:\nJulia Wrobel, PhD\nEmail:\njulia.wrobel@emory.edu\nSchool Address / Mailbox:\n1518 Clifton Rd, 1518-002-3AA\nOffice Hours:\nThursdays 1:00 PM- 2:00 PM\nGCR Room 210\nTeaching Assistant:\nWeijia Qian (weijia.qian@emory.edu)\nTA Office Hours: Tuesdays 11:00 AM - 12:00 PM GCR Room 369"
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "BIOS 731: Advanced Statistical Computing",
    "section": "Course Description",
    "text": "Course Description\nThis course, designed for Ph.D. biostatistics students, dives into the methods and applications of essential statistical computing techniques. Students will achieve two key goals:\n1. Master the implementation of algorithms behind widely used statistical models\n2. Build fluency in reproducible statistical programming\nWith a focus on practical implementation, the course balances foundational theory with hands-on computational experience. Topics include random number generation, bootstrap methods, optimization techniques, Expectation-Maximization (EM), Minorization-Maximization (MM), and Markov chain Monte Carlo (MCMC). On the programming side, students will explore simulation design, cluster computing, code profiling, parallelization, and R package development.\nThis course requires a significant amount of programming. Class examples and homework assignments will be predominantly in R. Because R can be relatively slow, students will learn how to optimize R for computational efficiency through code profiling and parallelization, and how to implement faster C++ backends using Rcpp.\nFor standard algorithms such as Newton–Raphson, EM, iteratively reweighted least squares, and Gibbs sampling, homework assignments will require students to directly implement the algorithm rather than use existing software packages. For more advanced topics—including ADMM, variational inference, and deep learning—assignments will focus on reading and summarizing journal articles and performing analyses using existing software.\nThe final project allows students to apply tools learned in class to topics related to their dissertation research.\nPrerequisites: BIOS 707, BIOS 709, or permission of the instructor.\nThe course will make heavy use of matrix algebra, including vector and matrix derivatives, norms, eigenvalues and eigenvectors, and matrix inverses.\nR is required for homework assignments unless otherwise specified."
  },
  {
    "objectID": "syllabus.html#course-learning-objectives",
    "href": "syllabus.html#course-learning-objectives",
    "title": "BIOS 731: Advanced Statistical Computing",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nUpon successful completion of the course, students should be able to:\n\nUnderstand the theoretical and computational foundations of algorithms covered in the course\n\nImplement algorithms using high-level programming languages\n\nApply methods to real-world data\n\nWrite efficient, reproducible R code locally and on computing clusters\n\n\nPhD Competency Assessed\n\nDevelop and assess new statistical methods\n\nHomework 1 involves designing and implementing a simulation study.\n\nConduct complex statistical analyses\n\nThe final project applies a recent computational method to a real dataset.\n\nConduct independent research in biostatistics\n\nThe final exam involves reading a research paper and implementing the proposed method."
  },
  {
    "objectID": "syllabus.html#evaluation",
    "href": "syllabus.html#evaluation",
    "title": "BIOS 731: Advanced Statistical Computing",
    "section": "Evaluation",
    "text": "Evaluation\nEvaluation will be based on the following components:\n\n\n\nAssessment\nPercentage\n\n\n\n\nHomework (7 assignments)\n50%\n\n\nFinal Project and Presentation\n30%\n\n\nDiscussion Board (4 posts)\n10%\n\n\nParticipation\n10%\n\n\n\n\nGrading Scale\n\n\n\nGrade\nPercentage\nPoints\n\n\n\n\nA\n93–100%\n4.0\n\n\nA−\n90–93%\n3.7\n\n\nB+\n87–90%\n3.3\n\n\nB\n83–87%\n3.0\n\n\nB−\n80–83%\n2.7\n\n\nC\n65–80%\n2.0\n\n\nF\n&lt;65%\n0.0"
  },
  {
    "objectID": "syllabus.html#homework-50",
    "href": "syllabus.html#homework-50",
    "title": "BIOS 731: Advanced Statistical Computing",
    "section": "Homework (50%)",
    "text": "Homework (50%)\nHomework must be neat, well organized, and typed. Raw computer output is not acceptable. Numerical output should be incorporated into text or tables, and plots must be clearly labeled.\nStudents may collaborate, but submitted work must be their own and not identical to others.\nLate assignments receive a maximum of half credit. Assignments more than three days late will not be graded."
  },
  {
    "objectID": "syllabus.html#participation-10",
    "href": "syllabus.html#participation-10",
    "title": "BIOS 731: Advanced Statistical Computing",
    "section": "Participation (10%)",
    "text": "Participation (10%)\nStudents are expected to attend class in person. Participation credit requires attendance and engagement in at least 20 lectures through asking or answering questions."
  },
  {
    "objectID": "syllabus.html#final-project-and-presentation-30",
    "href": "syllabus.html#final-project-and-presentation-30",
    "title": "BIOS 731: Advanced Statistical Computing",
    "section": "Final Project and Presentation (30%)",
    "text": "Final Project and Presentation (30%)\nStudents will select a computational method from the past five years, apply it to a dataset, and present results to the class.\n\n\n\nComponent\nWeight\n\n\n\n\nWritten report\n50%\n\n\nIn-class presentation\n50%"
  },
  {
    "objectID": "syllabus.html#discussion-board-10",
    "href": "syllabus.html#discussion-board-10",
    "title": "BIOS 731: Advanced Statistical Computing",
    "section": "Discussion Board (10%)",
    "text": "Discussion Board (10%)\nFour discussion posts based on readings and/or seminar lectures."
  },
  {
    "objectID": "syllabus.html#course-structure",
    "href": "syllabus.html#course-structure",
    "title": "BIOS 731: Advanced Statistical Computing",
    "section": "Course Structure",
    "text": "Course Structure\nThis course meets in person twice weekly. The first half of each class is lecture-based, and the second half focuses on problem-solving in small groups."
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "BIOS 731: Advanced Statistical Computing",
    "section": "Course Policies",
    "text": "Course Policies\n\nAttendance\nRegular attendance is expected. Classes will be recorded and posted on Canvas.\n\n\nReadings and Texts\nNo required textbook. Suggested references include:\n\nMatloff, The Art of R Programming\nWickham, Advanced R\nBoyd & Vandenberghe, Convex Optimization\nHastie et al., Statistical Learning with Sparsity\nHastie et al., The Elements of Statistical Learning\nGolub & Van Loan, Matrix Computations\n\n\n\nInclusivity\nThis course values diverse perspectives. Please share your preferred name and pronouns."
  },
  {
    "objectID": "syllabus.html#rsph-policies",
    "href": "syllabus.html#rsph-policies",
    "title": "BIOS 731: Advanced Statistical Computing",
    "section": "RSPH Policies",
    "text": "RSPH Policies\n\nAccessibility and Accommodations\nStudents requiring accommodations must register with the Office of Accessibility Services.\n\n\nHonor Code and Academic Integrity\nStudents are expected to uphold the Laney Graduate School Honor Code."
  },
  {
    "objectID": "syllabus.html#class-schedule-tentative",
    "href": "syllabus.html#class-schedule-tentative",
    "title": "BIOS 731: Advanced Statistical Computing",
    "section": "Class Schedule (Tentative)",
    "text": "Class Schedule (Tentative)\n\n\n\n\n\n\n\n\n\nDate\nModule\nTopic\nAssignment\n\n\n\n\n1/14\nReproducible Computing\nGit/GitHub, reproducible R\n—\n\n\n1/15\nReproducible Computing\nDebugging, profiling\n\n\n\n1/21\nSimulation & Resampling\nSimulation studies\nHW1 due\n\n\n1/22\nSimulation & Resampling\nBootstrap & permutation tests\n\n\n\n1/28\nReproducible Computing\nParallelization, parametric reports\n\n\n\n1/29\n—\nNo class\n\n\n\n2/4\nOptimization I\nIntroduction to optimization\n\n\n\n2/5\nOptimization I\nGradient methods, GLMs\n\n\n\n2/11\nOptimization I\nEM I: applications\nHW2 due\n\n\n2/18\nOptimization I\nEM II: theory\n\n\n\n2/19\nOptimization I\nMM algorithm\n\n\n\n2/25\nBayesian Computing\nNumerical integration & sampling\n\n\n\n2/26\nBayesian Computing\nIntroduction to MCMC\nHW3 due\n\n\n3/4\nBayesian Computing\nMCMC II\n\n\n\n3/5\nBayesian Computing\nVariational Bayes\nProject proposals due\n\n\n3/11\nSpring Break\nNo class\n\n\n\n3/12\nSpring Break\nNo class\n\n\n\n3/18\nENAR\nNo class\n\n\n\n3/19\nCloud Computing\nHPC usage (guest lecture)\nHW4 due\n\n\n3/25\nCloud Computing\nCluster project organization\n\n\n\n3/26\nR Packages\nPackage basics\nHW5 due\n\n\n4/1\nR Packages\nRcpp\n\n\n\n4/2\nOptimization II\nLP, quantile regression\nHW6 due\n\n\n4/8\nOptimization II\nQP, LASSO\n\n\n\n4/9\nOptimization II\nHMM and ADMM\n\n\n\n4/15\nOptimization II\nStochastic gradient descent\n\n\n\n4/16\nSpecial Topics\nTBD\nHW7 due\n\n\n4/22\nFinal Presentations\nFirst half\n\n\n\n4/23\nFinal Presentations\nSecond half\n\n\n\n5/1\nFinal Project Due"
  },
  {
    "objectID": "homework/topic_reproducible_computing/HW_reproducibility.html",
    "href": "homework/topic_reproducible_computing/HW_reproducibility.html",
    "title": "Homework 1: Reproducible Computing",
    "section": "",
    "text": "This assignment reinforces ideas in Module 1: Reproducible Computing in R. We focus specifically on project organization and GitHub."
  },
  {
    "objectID": "homework/topic_reproducible_computing/HW_reproducibility.html#context",
    "href": "homework/topic_reproducible_computing/HW_reproducibility.html#context",
    "title": "Homework 1: Reproducible Computing",
    "section": "",
    "text": "This assignment reinforces ideas in Module 1: Reproducible Computing in R. We focus specifically on project organization and GitHub."
  },
  {
    "objectID": "homework/topic_reproducible_computing/HW_reproducibility.html#due-date-and-submission",
    "href": "homework/topic_reproducible_computing/HW_reproducibility.html#due-date-and-submission",
    "title": "Homework 1: Reproducible Computing",
    "section": "Due date and submission",
    "text": "Due date and submission\nPlease submit (via Canvas) the web address of the GitHub repo containing your work for this assignment; git commits after the due date will cause the assignment to be considered late.\nR Markdown or Quarto documents included as part of your solutions must not install packages, and should only load the packages necessary for your submission to knit."
  },
  {
    "objectID": "homework/topic_reproducible_computing/HW_reproducibility.html#points",
    "href": "homework/topic_reproducible_computing/HW_reproducibility.html#points",
    "title": "Homework 1: Reproducible Computing",
    "section": "Points",
    "text": "Points\n\n\n\n\n\nProblem\nPoints\n\n\n\n\nProblem 0\n20\n\n\nProblem 1.1\n10\n\n\nProblem 1.2\n5\n\n\nProblem 1.3\n20\n\n\nProblem 1.4\n30\n\n\nProblem 1.5\n15"
  },
  {
    "objectID": "homework/topic_reproducible_computing/HW_reproducibility.html#problem-1",
    "href": "homework/topic_reproducible_computing/HW_reproducibility.html#problem-1",
    "title": "Homework 1: Reproducible Computing",
    "section": "Problem 1",
    "text": "Problem 1\nThis “problem” focuses on structure of your submission, especially the use git and GitHub for reproducibility, R Projects to organize your work, R Markdown/Quarto to write reproducible reports, relative paths to load data from local files, and reasonable naming structures for your files.\nTo that end:\n\ncreate a public GitHub repo + local R Project; I suggest naming this repo / directory bios731_hw1_YourLastName (e.g. bios731_hw1_wrobel for Julia)\nSubmit your whole project folder to GitHub\nSubmit a PDF knitted from Rmd to Canvas. Your solutions to the problem here should be implemented in your .Rmd file, and your git commit history should reflect the process you used to solve these Problems.\n\n\nCreate a reproducible project, using the project structure learned in the project organization structure.\nPick a dataset and do a brief analysis with this dataset following structure from project organization. You may want to use a dataset from your own research, however your project will need to be fully reproducible and hosted publicly on GitHub, so please make sure your dataset can be uploaded to GH before you do so! There should be a data cleaning component, and modeling component, a data visualization component, and a final report.\nYour project should have a README that describes the dataset as well as the files within your project folder.\nPut your project in a public repository on GitHub, please use informative commit messages.\nYour final product should be fully reproducible. Your grade will depend on it. We should be able to clone your repo from GitHub and follow the readme to reproduce the workflow without changing any filepaths or any thing else!."
  },
  {
    "objectID": "homework/topic_reproducible_computing/HW_reproducibility.html#rubric",
    "href": "homework/topic_reproducible_computing/HW_reproducibility.html#rubric",
    "title": "Homework 1: Reproducible Computing",
    "section": "Rubric",
    "text": "Rubric\n\nNeed to have Rproject\nShould have informative commit messages\nShould have code that is self documenting\nShould have a README explaining the workflow\nIf code doesn’t knit because we need to install a library first, no points should be deducted; however libraries should be loaded at the beginning of the document and readme should include sessionInfo()"
  },
  {
    "objectID": "topic_optimization.html",
    "href": "topic_optimization.html",
    "title": "Optimization, Part 1",
    "section": "",
    "text": "Is this module we will first cover convex optimization and gradient descent. In the second week of this module we will move to the EM and MM algorithms.\n\nIntroduction to optimization and gradient descent\nEM 1: Applications\nEM 2: Theory and Inference\nMM Algorithm"
  },
  {
    "objectID": "content_repro.html",
    "href": "content_repro.html",
    "title": "A Crash Course in Reproducible Computing",
    "section": "",
    "text": "In this, we will cover basic topics in reproducible computing, including coding style, project organization, and git/GitHub.\n\nSlide Deck: project organizationSlide Deck: git/GithubLab: projectrLab: CDC open data\n\n\n\nCrash course on reproducible computing in R\nDownload slides (PDF)\n\n\n\n\nCrash course on reproducible computing in R\nDownload slides (PDF)\n\n\n\nI asked ChatGPT to write me some code for creating a reproducible file structure for a new project. The prompt I used is below:\n\nCan you write me an R function that will take in a filepath, and name for a new directlry then create a new directory with that same that includes subfolders called “analysis”, “source”, “data”, “results”, and “literature”, and also contains an initialized .git repo, a .Rproj file with the same name as the new directory in the root, and a README.md file?\n\nBelow is the code it wrote me. What is it doing? Modify it to suit your own preferences, and see if it works!\n\nAlso, we just spent a while talking about coding practices. What do you like about this code? What do you dislike?\n\n\ncreate_r_project_skeleton &lt;- function(base_path,\n                                      project_name,\n                                      open_rproj = FALSE,\n                                      git = TRUE) {\n  stopifnot(is.character(base_path), length(base_path) == 1, nzchar(base_path))\n  stopifnot(is.character(project_name), length(project_name) == 1, nzchar(project_name))\n\n  # Normalize inputs\n  base_path &lt;- normalizePath(base_path, winslash = \"/\", mustWork = FALSE)\n\n  # Basic name safety (avoid characters that are awkward on Windows/macOS/Linux)\n  if (grepl(\"[/\\\\\\\\:*?\\\"&lt;&gt;|]\", project_name)) {\n    stop(\"project_name contains invalid path characters. Use a simple folder name.\")\n  }\n\n  project_path &lt;- file.path(base_path, project_name)\n\n  if (dir.exists(project_path)) {\n    stop(\"Target project directory already exists: \", project_path)\n  }\n\n  # Create main directory\n  ok &lt;- dir.create(project_path, recursive = TRUE, showWarnings = FALSE)\n  if (!ok || !dir.exists(project_path)) {\n    stop(\"Failed to create project directory: \", project_path)\n  }\n\n  # Create subdirectories\n  subdirs &lt;- c(\"analysis\", \"source\", \"data\", \"results\", \"literature\")\n  for (d in subdirs) {\n    dir.create(file.path(project_path, d), showWarnings = FALSE)\n  }\n\n  # Create .Rproj file\n  rproj_path &lt;- file.path(project_path, paste0(project_name, \".Rproj\"))\n  rproj_contents &lt;- c(\n    \"Version: 1.0\",\n    \"\",\n    \"RestoreWorkspace: No\",\n    \"SaveWorkspace: No\",\n    \"AlwaysSaveHistory: Default\",\n    \"\",\n    \"EnableCodeIndexing: Yes\",\n    \"UseSpacesForTab: Yes\",\n    \"NumSpacesForTab: 2\",\n    \"Encoding: UTF-8\",\n    \"\",\n    \"RnwWeave: Sweave\",\n    \"LaTeX: pdfLaTeX\"\n  )\n  writeLines(rproj_contents, con = rproj_path, useBytes = TRUE)\n\n  # Create README.md\n  readme_path &lt;- file.path(project_path, \"README.md\")\n  readme_contents &lt;- c(\n    paste0(\"# \", project_name),\n    \"\",\n    \"## Overview\",\n    \"Describe the purpose of this project.\",\n    \"\",\n    \"## Structure\",\n    \"- `analysis/` - analysis scripts and notebooks\",\n    \"- `source/` - reusable functions and modules\",\n    \"- `data/` - raw and intermediate data (consider a data management policy)\",\n    \"- `results/` - figures, tables, and outputs\",\n    \"- `literature/` - papers, notes, and references\",\n    \"\",\n    \"## Reproducibility\",\n    \"- Record package versions (e.g., `renv::init()`) as needed.\"\n  )\n  writeLines(readme_contents, con = readme_path, useBytes = TRUE)\n\n  # Initialize git repo (optional)\n  if (isTRUE(git)) {\n    git_available &lt;- nzchar(Sys.which(\"git\"))\n    if (!git_available) {\n      warning(\"git=TRUE but Git executable not found on PATH; skipping git init.\")\n    } else {\n      old_wd &lt;- getwd()\n      on.exit(setwd(old_wd), add = TRUE)\n      setwd(project_path)\n\n      # Initialize repository\n      system2(\"git\", c(\"init\"), stdout = TRUE, stderr = TRUE)\n\n      # Optional: create a minimal .gitignore tailored to typical R projects\n      gitignore_path &lt;- file.path(project_path, \".gitignore\")\n      gitignore_contents &lt;- c(\n        \".Rhistory\",\n        \".RData\",\n        \".Ruserdata\",\n        \".DS_Store\",\n        \"Thumbs.db\",\n        \"results/\",\n        \"data/\"\n      )\n      writeLines(gitignore_contents, con = gitignore_path, useBytes = TRUE)\n    }\n  }\n\n  # Optionally open the project in RStudio if available\n  if (isTRUE(open_rproj)) {\n    if (interactive() && nzchar(Sys.which(\"rstudio\"))) {\n      system2(\"rstudio\", rproj_path, wait = FALSE)\n    } else if (interactive()) {\n      message(\"open_rproj=TRUE but could not find 'rstudio' on PATH; created: \", rproj_path)\n    }\n  }\n\n  invisible(list(\n    project_path = project_path,\n    subdirs = file.path(project_path, subdirs),\n    rproj = rproj_path,\n    readme = readme_path,\n    git_initialized = isTRUE(git) && nzchar(Sys.which(\"git\"))\n  ))\n}\n\n\n\nThe CDC has over 1300 open-source datasets available at data.cdc.gov.\n\nTopic areas include injury & violence, vaccination, smoking,pregnancy, chronic disease, and disease surveillance\nGreat source of Covid surveillance data\ndata.gov hosts additional 250K+ datasets\n\nThe Covid Wastewater dataset comes from the National Wastewater Surveillance System(NWSS) Public SARS-CoV-2 Concentration in Wastewater Data.\n\nContains SARS-CoV-2 concentration at different sampling locations across the U.S.\nUpdate Daily\n\nThere are two available datasets:\n\nLongitudinal data, which has four columns and provides concentrations over time\nCross-sectional data, which includes current concentration and other summaries, as well as information about state, county (16 total columns)\n\nOur goal is to produce an end-to-end reproducible workflow with this data. We will do the following:\n\nDownload the data directly into R by interacting with the CDC API.\nClean the data: Longitudinal data contains concentrations over time,and cross-sectional data contains information about county each data collection site is located in. We will merge these two datasets and subset to collection sites in Georgia only.\nAnalyze data.\nVisualize data.\n\nPull these steps together into a documented reproducible report.\n\n\nI’ve done most of these steps for us already by creating an example project folder. Download it here.\n\nDownload and unzip this folder\nCheck out the files in the source subfolder. What are each of these doing?\nBuild a final_report Quarto or R Markdown document that calls each of the steps of the data analysis, and explains what is being done at each step.\nAs you make changes to the project, commit changes using git.\nFeel free to add some of your own visualizations.\n\n\n\n\n\nPush your finished project to GitHub\nClone one of your classmate’s repos and try to run their final report document. Was it fully reproducible?"
  },
  {
    "objectID": "content_repro.html#overview",
    "href": "content_repro.html#overview",
    "title": "A Crash Course in Reproducible Computing",
    "section": "",
    "text": "In this, we will cover basic topics in reproducible computing, including coding style, project organization, and git/GitHub.\n\nSlide Deck: project organizationSlide Deck: git/GithubLab: projectrLab: CDC open data\n\n\n\nCrash course on reproducible computing in R\nDownload slides (PDF)\n\n\n\n\nCrash course on reproducible computing in R\nDownload slides (PDF)\n\n\n\nI asked ChatGPT to write me some code for creating a reproducible file structure for a new project. The prompt I used is below:\n\nCan you write me an R function that will take in a filepath, and name for a new directlry then create a new directory with that same that includes subfolders called “analysis”, “source”, “data”, “results”, and “literature”, and also contains an initialized .git repo, a .Rproj file with the same name as the new directory in the root, and a README.md file?\n\nBelow is the code it wrote me. What is it doing? Modify it to suit your own preferences, and see if it works!\n\nAlso, we just spent a while talking about coding practices. What do you like about this code? What do you dislike?\n\n\ncreate_r_project_skeleton &lt;- function(base_path,\n                                      project_name,\n                                      open_rproj = FALSE,\n                                      git = TRUE) {\n  stopifnot(is.character(base_path), length(base_path) == 1, nzchar(base_path))\n  stopifnot(is.character(project_name), length(project_name) == 1, nzchar(project_name))\n\n  # Normalize inputs\n  base_path &lt;- normalizePath(base_path, winslash = \"/\", mustWork = FALSE)\n\n  # Basic name safety (avoid characters that are awkward on Windows/macOS/Linux)\n  if (grepl(\"[/\\\\\\\\:*?\\\"&lt;&gt;|]\", project_name)) {\n    stop(\"project_name contains invalid path characters. Use a simple folder name.\")\n  }\n\n  project_path &lt;- file.path(base_path, project_name)\n\n  if (dir.exists(project_path)) {\n    stop(\"Target project directory already exists: \", project_path)\n  }\n\n  # Create main directory\n  ok &lt;- dir.create(project_path, recursive = TRUE, showWarnings = FALSE)\n  if (!ok || !dir.exists(project_path)) {\n    stop(\"Failed to create project directory: \", project_path)\n  }\n\n  # Create subdirectories\n  subdirs &lt;- c(\"analysis\", \"source\", \"data\", \"results\", \"literature\")\n  for (d in subdirs) {\n    dir.create(file.path(project_path, d), showWarnings = FALSE)\n  }\n\n  # Create .Rproj file\n  rproj_path &lt;- file.path(project_path, paste0(project_name, \".Rproj\"))\n  rproj_contents &lt;- c(\n    \"Version: 1.0\",\n    \"\",\n    \"RestoreWorkspace: No\",\n    \"SaveWorkspace: No\",\n    \"AlwaysSaveHistory: Default\",\n    \"\",\n    \"EnableCodeIndexing: Yes\",\n    \"UseSpacesForTab: Yes\",\n    \"NumSpacesForTab: 2\",\n    \"Encoding: UTF-8\",\n    \"\",\n    \"RnwWeave: Sweave\",\n    \"LaTeX: pdfLaTeX\"\n  )\n  writeLines(rproj_contents, con = rproj_path, useBytes = TRUE)\n\n  # Create README.md\n  readme_path &lt;- file.path(project_path, \"README.md\")\n  readme_contents &lt;- c(\n    paste0(\"# \", project_name),\n    \"\",\n    \"## Overview\",\n    \"Describe the purpose of this project.\",\n    \"\",\n    \"## Structure\",\n    \"- `analysis/` - analysis scripts and notebooks\",\n    \"- `source/` - reusable functions and modules\",\n    \"- `data/` - raw and intermediate data (consider a data management policy)\",\n    \"- `results/` - figures, tables, and outputs\",\n    \"- `literature/` - papers, notes, and references\",\n    \"\",\n    \"## Reproducibility\",\n    \"- Record package versions (e.g., `renv::init()`) as needed.\"\n  )\n  writeLines(readme_contents, con = readme_path, useBytes = TRUE)\n\n  # Initialize git repo (optional)\n  if (isTRUE(git)) {\n    git_available &lt;- nzchar(Sys.which(\"git\"))\n    if (!git_available) {\n      warning(\"git=TRUE but Git executable not found on PATH; skipping git init.\")\n    } else {\n      old_wd &lt;- getwd()\n      on.exit(setwd(old_wd), add = TRUE)\n      setwd(project_path)\n\n      # Initialize repository\n      system2(\"git\", c(\"init\"), stdout = TRUE, stderr = TRUE)\n\n      # Optional: create a minimal .gitignore tailored to typical R projects\n      gitignore_path &lt;- file.path(project_path, \".gitignore\")\n      gitignore_contents &lt;- c(\n        \".Rhistory\",\n        \".RData\",\n        \".Ruserdata\",\n        \".DS_Store\",\n        \"Thumbs.db\",\n        \"results/\",\n        \"data/\"\n      )\n      writeLines(gitignore_contents, con = gitignore_path, useBytes = TRUE)\n    }\n  }\n\n  # Optionally open the project in RStudio if available\n  if (isTRUE(open_rproj)) {\n    if (interactive() && nzchar(Sys.which(\"rstudio\"))) {\n      system2(\"rstudio\", rproj_path, wait = FALSE)\n    } else if (interactive()) {\n      message(\"open_rproj=TRUE but could not find 'rstudio' on PATH; created: \", rproj_path)\n    }\n  }\n\n  invisible(list(\n    project_path = project_path,\n    subdirs = file.path(project_path, subdirs),\n    rproj = rproj_path,\n    readme = readme_path,\n    git_initialized = isTRUE(git) && nzchar(Sys.which(\"git\"))\n  ))\n}\n\n\n\nThe CDC has over 1300 open-source datasets available at data.cdc.gov.\n\nTopic areas include injury & violence, vaccination, smoking,pregnancy, chronic disease, and disease surveillance\nGreat source of Covid surveillance data\ndata.gov hosts additional 250K+ datasets\n\nThe Covid Wastewater dataset comes from the National Wastewater Surveillance System(NWSS) Public SARS-CoV-2 Concentration in Wastewater Data.\n\nContains SARS-CoV-2 concentration at different sampling locations across the U.S.\nUpdate Daily\n\nThere are two available datasets:\n\nLongitudinal data, which has four columns and provides concentrations over time\nCross-sectional data, which includes current concentration and other summaries, as well as information about state, county (16 total columns)\n\nOur goal is to produce an end-to-end reproducible workflow with this data. We will do the following:\n\nDownload the data directly into R by interacting with the CDC API.\nClean the data: Longitudinal data contains concentrations over time,and cross-sectional data contains information about county each data collection site is located in. We will merge these two datasets and subset to collection sites in Georgia only.\nAnalyze data.\nVisualize data.\n\nPull these steps together into a documented reproducible report.\n\n\nI’ve done most of these steps for us already by creating an example project folder. Download it here.\n\nDownload and unzip this folder\nCheck out the files in the source subfolder. What are each of these doing?\nBuild a final_report Quarto or R Markdown document that calls each of the steps of the data analysis, and explains what is being done at each step.\nAs you make changes to the project, commit changes using git.\nFeel free to add some of your own visualizations.\n\n\n\n\n\nPush your finished project to GitHub\nClone one of your classmate’s repos and try to run their final report document. Was it fully reproducible?"
  },
  {
    "objectID": "topic_R_packages.html",
    "href": "topic_R_packages.html",
    "title": "Building R Packages",
    "section": "",
    "text": "In progress."
  },
  {
    "objectID": "topic_reproducible_computing.html",
    "href": "topic_reproducible_computing.html",
    "title": "Reproducible Computing",
    "section": "",
    "text": "Covers reproducible computing in R, briefly GitHub, and some best practices for efficient coding in R\n\nReproducible computing, Project organization/Version control\nReproducible computing, Vectorization, code profiling, and parallelization\n\nThis paper on good enough practices for scientific computing gives an idea of what we want to achieve in this course, and why we want to achieve it.\nAlso, check out this editorial I co-wrote as one of the Associate Editor’s for Reproducibility at JASA. It describes the JASA reproducibility process."
  },
  {
    "objectID": "labs/lab_optimization_solutions.html",
    "href": "labs/lab_optimization_solutions.html",
    "title": "Lab: a crash course in reproducibility",
    "section": "",
    "text": "Load your libraries here. I find it to be best practice to load libraries at the beginning of a document. Why?"
  },
  {
    "objectID": "labs/lab_optimization_solutions.html#exercise-1",
    "href": "labs/lab_optimization_solutions.html#exercise-1",
    "title": "Lab: a crash course in reproducibility",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nTry out the steepest descent algorithm with different x0 and alpha values\n\nFor each x0 and alpha value, plot the evolution of \\(x\\) with each iteration\nFor each x0 and alpha value, plot the evolution of \\(f'(x)\\) with each iteration\n\nWhat is the solution?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1] 8.334836\n\n\n[1] 244\n\n\n[1] TRUE"
  },
  {
    "objectID": "labs/lab_optimization_solutions.html#exercise-2",
    "href": "labs/lab_optimization_solutions.html#exercise-2",
    "title": "Lab: a crash course in reproducibility",
    "section": "Exercise 2",
    "text": "Exercise 2\nModify the steepest_descent() function above to use Newtons method for adjusting step size and call this new function newton().\n\nHow does newton() compare with steepest_descent() in terms of number of iterations?\nHow does newton() compare with steepest_descent() in terms of computation time?\n\n\n\n[1] 8.334836\n\n\n\n\n\n\n\n\n\n[1] 8.334836\n\n\nLet’s also plot the hessian"
  },
  {
    "objectID": "content_optimization.html",
    "href": "content_optimization.html",
    "title": "Convex optimization",
    "section": "",
    "text": "In this, we will introduce to convex optimization and gradient descent.\n\nSlide Deck: Introduction to optimizationSlide Deck: Gradient methodsLab: Introduction to optimizationLab: Gradient descent\n\n\n\nIntro optimization\nDownload slides (PDF)\n\n\n\n\nGradient Methods\nDownload slides (PDF)\n\n\n\n\n\nFor simplicity, we’ll focus on a one-dimensional example.\n\\[\nf(x) = (x-50)^2 + e^x/50, \\quad f'(x) = 2x - 100 + e^x/50 = 0\n\\]\nThe R function below, steepest_descent(), implements this algorithm for \\(f(x)\\).\n\n\n\nTry out the steepest descent algorithm with different x0 and alpha values\n\nFor each x0 and alpha value, plot the evolution of \\(x\\) with each iteration\nFor each x0 and alpha value, plot the evolution of \\(f'(x)\\) with each iteration\n\nWhat is the solution?\n\nNewton’s method\nWe’ll use the same example as from the steepest descent algorithm.\n\\[f(x) = (x-50)^2 + e^x/50, \\quad f'(x) = 2x - 100 + e^x/50 = 0\\]\n\n\\(f''(x) = 2 + e^x/50\\)\n\n\n\n\nModify the steepest_descent() function above to use Newtons method for adjusting step size and call this new function newton().\n\nHow does newton() compare with steepest_descent() in terms of number of iterations?\nHow does newton() compare with steepest_descent() in terms of computation time?\n\n\n\n\n\n\n\n\n\\[\\log(E[Y_i|X_i]) = X_i^T\\beta \\]\n\n\\(Y_i \\sim Poisson(\\mu_i)\\)\n\\(Var(\\mu_i) = \\mu_i\\)\n\\(g'(\\mu_i) = \\frac{1}{\\mu_i}\\)\n\n\\[l(\\beta) = \\sum_i \\left( y_iX_i^T\\beta-e^{X_i^T\\beta}-\\log y_i!\\right)\\]\n\ngradient: \\(\\frac{\\partial l(\\beta)}{\\partial \\beta} = \\sum_i(Y_i - e^{X_i^T\\beta})X_i\\)\nHessian: \\(\\frac{\\partial^2 l(\\beta)}{\\partial \\beta^2} = -\\sum_i e^{X_i^T\\beta} X_i^TX_i\\)\n\n\n\nThe function newton() below performs Newton’s method for the lab example from earlier. Make a new function, newton_poisson(), to find MLE estimates for \\(\\beta\\) in Poisson regression. Use a simulated dataset with the following parameters:\nSimulation study specification:\n\nsimulate from the model \\(log(E(Y_i = 1|X_i)) = \\beta_0 + \\beta_1X_i\\)\n\n\\(\\beta_0 = 1\\)\n\\(\\beta_1 = 0.3\\)\n\\(X_i \\sim N(0, 1)\\)\n\\(n = 100\\)\n\\(nsim = 1\\)\n\n\nTry a couple starting parameters for beta, including both good (close to the ground truth) and bad values.\n\nsimulate from the model \\(log(E(Y_i = 1|X_i)) = \\beta_0 + \\beta_1X_i\\)\n\n\\(\\beta_0 = 1\\)\n\\(\\beta_1 = 0.3\\)\n\\(X_i \\sim N(0, 1)\\)\n\\(n = 100\\)\n\\(nsim = 1\\)\n\n\n\n\nConverged in 21 iterations.\n\n\n           [,1]\n[1,]  1.3664363\n[2,] -0.0286162\n\n\n[1] 0.05113433 0.04826915\n\n\n\nCall:\nglm(formula = y ~ x, family = poisson)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.36644    0.05113  26.723   &lt;2e-16 ***\nx           -0.02862    0.04827  -0.593    0.553    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 93.889  on 99  degrees of freedom\nResidual deviance: 93.538  on 98  degrees of freedom\nAIC: 408.25\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nModify your newton_poisson() function to return a 95% confidence interval for \\(\\beta\\). How do your results compare to glm()?\n\n\nfunction (x, y = NULL, na.rm = FALSE, use) \n{\n    if (missing(use)) \n        use &lt;- if (na.rm) \n            \"na.or.complete\"\n        else \"everything\"\n    na.method &lt;- pmatch(use, c(\"all.obs\", \"complete.obs\", \"pairwise.complete.obs\", \n        \"everything\", \"na.or.complete\"))\n    if (is.na(na.method)) \n        stop(\"invalid 'use' argument\")\n    if (is.data.frame(x)) \n        x &lt;- as.matrix(x)\n    else if (!is.null(x)) \n        stopifnot(is.atomic(x))\n    if (is.data.frame(y)) \n        y &lt;- as.matrix(y)\n    else if (!is.null(y)) \n        stopifnot(is.atomic(y))\n    .Call(C_cov, x, y, na.method, FALSE)\n}\n&lt;bytecode: 0x1123d0af0&gt;\n&lt;environment: namespace:stats&gt;\n\n\n\n\n\n\nThis is the beginning of HW 2.\nFor a given subject in a study, we are interested in modeling \\(\\pi_i = P(Y_i = 1|X_i = x_i)\\), where \\(Y_i \\in \\{0, 1\\}\\). The logistic regression model takes the form\n\n\\[\\text{logit}(\\pi_i) = \\log \\left(\\frac{\\pi_i}{1-\\pi_i}\\right) = \\log\\left({\\frac{P(Y_i = 1|X_i)}{1-P(Y_i = 1|X_i)}}\\right) = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\ldots + \\beta_pX_{pi}\\]\n\n\\(Y_1, Y_2,\\ldots, Y_n \\sim Bernoulli(\\pi)\\)\nPDF is \\(f(y_i; \\pi) = \\pi^{y_i}(1-\\pi)^{1-y_i}\\)\n\n\n\n\nDerive likelihood, gradient, and Hessian for logistic regression for an arbitrary number of predictors \\(p\\)\nWhat is the Newton’s method update for \\(\\beta\\) for logistic regression?\nIs logistic regression a convex optimization problem? Why or why not?"
  },
  {
    "objectID": "content_optimization.html#overview",
    "href": "content_optimization.html#overview",
    "title": "Convex optimization",
    "section": "",
    "text": "In this, we will introduce to convex optimization and gradient descent.\n\nSlide Deck: Introduction to optimizationSlide Deck: Gradient methodsLab: Introduction to optimizationLab: Gradient descent\n\n\n\nIntro optimization\nDownload slides (PDF)\n\n\n\n\nGradient Methods\nDownload slides (PDF)\n\n\n\n\n\nFor simplicity, we’ll focus on a one-dimensional example.\n\\[\nf(x) = (x-50)^2 + e^x/50, \\quad f'(x) = 2x - 100 + e^x/50 = 0\n\\]\nThe R function below, steepest_descent(), implements this algorithm for \\(f(x)\\).\n\n\n\nTry out the steepest descent algorithm with different x0 and alpha values\n\nFor each x0 and alpha value, plot the evolution of \\(x\\) with each iteration\nFor each x0 and alpha value, plot the evolution of \\(f'(x)\\) with each iteration\n\nWhat is the solution?\n\nNewton’s method\nWe’ll use the same example as from the steepest descent algorithm.\n\\[f(x) = (x-50)^2 + e^x/50, \\quad f'(x) = 2x - 100 + e^x/50 = 0\\]\n\n\\(f''(x) = 2 + e^x/50\\)\n\n\n\n\nModify the steepest_descent() function above to use Newtons method for adjusting step size and call this new function newton().\n\nHow does newton() compare with steepest_descent() in terms of number of iterations?\nHow does newton() compare with steepest_descent() in terms of computation time?\n\n\n\n\n\n\n\n\n\\[\\log(E[Y_i|X_i]) = X_i^T\\beta \\]\n\n\\(Y_i \\sim Poisson(\\mu_i)\\)\n\\(Var(\\mu_i) = \\mu_i\\)\n\\(g'(\\mu_i) = \\frac{1}{\\mu_i}\\)\n\n\\[l(\\beta) = \\sum_i \\left( y_iX_i^T\\beta-e^{X_i^T\\beta}-\\log y_i!\\right)\\]\n\ngradient: \\(\\frac{\\partial l(\\beta)}{\\partial \\beta} = \\sum_i(Y_i - e^{X_i^T\\beta})X_i\\)\nHessian: \\(\\frac{\\partial^2 l(\\beta)}{\\partial \\beta^2} = -\\sum_i e^{X_i^T\\beta} X_i^TX_i\\)\n\n\n\nThe function newton() below performs Newton’s method for the lab example from earlier. Make a new function, newton_poisson(), to find MLE estimates for \\(\\beta\\) in Poisson regression. Use a simulated dataset with the following parameters:\nSimulation study specification:\n\nsimulate from the model \\(log(E(Y_i = 1|X_i)) = \\beta_0 + \\beta_1X_i\\)\n\n\\(\\beta_0 = 1\\)\n\\(\\beta_1 = 0.3\\)\n\\(X_i \\sim N(0, 1)\\)\n\\(n = 100\\)\n\\(nsim = 1\\)\n\n\nTry a couple starting parameters for beta, including both good (close to the ground truth) and bad values.\n\nsimulate from the model \\(log(E(Y_i = 1|X_i)) = \\beta_0 + \\beta_1X_i\\)\n\n\\(\\beta_0 = 1\\)\n\\(\\beta_1 = 0.3\\)\n\\(X_i \\sim N(0, 1)\\)\n\\(n = 100\\)\n\\(nsim = 1\\)\n\n\n\n\nConverged in 21 iterations.\n\n\n           [,1]\n[1,]  1.3664363\n[2,] -0.0286162\n\n\n[1] 0.05113433 0.04826915\n\n\n\nCall:\nglm(formula = y ~ x, family = poisson)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.36644    0.05113  26.723   &lt;2e-16 ***\nx           -0.02862    0.04827  -0.593    0.553    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 93.889  on 99  degrees of freedom\nResidual deviance: 93.538  on 98  degrees of freedom\nAIC: 408.25\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nModify your newton_poisson() function to return a 95% confidence interval for \\(\\beta\\). How do your results compare to glm()?\n\n\nfunction (x, y = NULL, na.rm = FALSE, use) \n{\n    if (missing(use)) \n        use &lt;- if (na.rm) \n            \"na.or.complete\"\n        else \"everything\"\n    na.method &lt;- pmatch(use, c(\"all.obs\", \"complete.obs\", \"pairwise.complete.obs\", \n        \"everything\", \"na.or.complete\"))\n    if (is.na(na.method)) \n        stop(\"invalid 'use' argument\")\n    if (is.data.frame(x)) \n        x &lt;- as.matrix(x)\n    else if (!is.null(x)) \n        stopifnot(is.atomic(x))\n    if (is.data.frame(y)) \n        y &lt;- as.matrix(y)\n    else if (!is.null(y)) \n        stopifnot(is.atomic(y))\n    .Call(C_cov, x, y, na.method, FALSE)\n}\n&lt;bytecode: 0x1123d0af0&gt;\n&lt;environment: namespace:stats&gt;\n\n\n\n\n\n\nThis is the beginning of HW 2.\nFor a given subject in a study, we are interested in modeling \\(\\pi_i = P(Y_i = 1|X_i = x_i)\\), where \\(Y_i \\in \\{0, 1\\}\\). The logistic regression model takes the form\n\n\\[\\text{logit}(\\pi_i) = \\log \\left(\\frac{\\pi_i}{1-\\pi_i}\\right) = \\log\\left({\\frac{P(Y_i = 1|X_i)}{1-P(Y_i = 1|X_i)}}\\right) = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\ldots + \\beta_pX_{pi}\\]\n\n\\(Y_1, Y_2,\\ldots, Y_n \\sim Bernoulli(\\pi)\\)\nPDF is \\(f(y_i; \\pi) = \\pi^{y_i}(1-\\pi)^{1-y_i}\\)\n\n\n\n\nDerive likelihood, gradient, and Hessian for logistic regression for an arbitrary number of predictors \\(p\\)\nWhat is the Newton’s method update for \\(\\beta\\) for logistic regression?\nIs logistic regression a convex optimization problem? Why or why not?"
  },
  {
    "objectID": "topic_cloud_computing.html",
    "href": "topic_cloud_computing.html",
    "title": "Cloud Computing",
    "section": "",
    "text": "In progress."
  },
  {
    "objectID": "topic_simulation.html",
    "href": "topic_simulation.html",
    "title": "Simulation and Resampling",
    "section": "",
    "text": "Covers best practices for simulation studies in the first lecture, then bootstrap and permutation tests in the second lecture\n\nSimulation studies/Resampling methods\nParallelization and other speedup tricks\n\nA couple of papers I highly recommend reading on these topics are:\n\nUsing simulation studies to evaluate statistical methods\nBootstrap"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BIOS 731: Advanced Statistical Computing",
    "section": "",
    "text": "This course is being offered in the Spring of 2026 through the Biostatistics and Informatics Department at the Emory University Rollins School of Public Health.\n\nInstructor: Julia Wrobel"
  },
  {
    "objectID": "topic_optimization_II.html",
    "href": "topic_optimization_II.html",
    "title": "Optimization, Part II",
    "section": "",
    "text": "In progress."
  },
  {
    "objectID": "content_simulations.html",
    "href": "content_simulations.html",
    "title": "Simulations and resampling methods",
    "section": "",
    "text": "In this, we will cover basic simulations, bootstrapping, permutation tests, and speeding up code through vectorization and parallelization.\n\nSlide Deck: SimulationsSlide Deck: BootstrapSlide Deck: Benchmarking and parallelizationLab: simulationsLab: resampling methodsLab: Benchmarking and parallelization\n\n\n\nSimulation studies\nDownload slides (PDF)\n\n\n\n\nResampling Methods\nDownload slides (PDF)\n\n\n\n\nSpeeding up your code in R\nDownload slides (PDF)\n\n\n\nSimulation study workflow\nFirst, use ADEMP guidelines to define the following\n\nSimulation scenarios\nEstimands/targets\nPerformance measures\n\nThen, for a given simulation scenario follow the basic steps:\n\nChoose \\(n_{sim}\\)\nSimulate data\nApply method(s)\nCalculate estimates\nStore results\n\nWhenever possible, separate scripts for performing different tasks.\n\n\nOur goal in this exercise will be to plan a well-organized simulation study for multiple linear regression. It will be similar in structure to your Homework 2. Below is a multiple linear regression model, where we are interested in primarily treatment effect.\n\\[Y_i = \\beta_0 + \\beta_{treatment}X_{i1} + \\mathbf{Z_i}^T\\boldsymbol{\\gamma} + \\epsilon_i\\]\n\n\\(Y_i\\): continuous outcome\n\\(X_{i1}\\): treatment group indicator; \\(X_{i1}=1\\) for treated\n\\(\\mathbf{Z_i}\\): vector of potential confounders\n\n\n\n\\(\\beta_{treatment}\\): average treatment effect, adjusting for \\(\\mathbf{Z_i}\\)\n\\(\\boldsymbol{\\gamma}\\): vector of regression coefficient values for confounders\n\\(\\epsilon_i\\): iid error, \\(\\sim N(0, \\sigma^2)\\)\n\nIn our simulation, we want to\n\nEstimate \\(\\beta_{treatment}\\)\n\nEvaluate through bias and coverage\n\nEstimate \\(\\sigma^2\\)\n\nEvaluate through bias\n\nEvaluate these properties at:\n\nSample size \\(n \\in \\{10, 50, 500\\}\\)\nTrue values \\(\\beta_{treatment} \\in \\{0.5, 2\\}\\)\nTrue values \\(\\sigma^2 \\in \\{1, 5\\}\\)\n\nAssume for now that there are no confounders (\\(\\boldsymbol{\\gamma} = 0\\))\nAssume \\(X_{i1} = 1\\) with probability 0.5\nUse a full factorial design\n\n\n\n\n\nHow many simulation scenarios will you be running?\nWhat are the estimand(s)\nWhat method(s) are being evaluated/compared?\nWhat are the performance measure(s)?\n\n\n\n\nBased on desired coverage of 90% with Monte Carlo error of no more than 1%, how many simulations (\\(n_{sim}\\)) should we perform for each simulation scenario?\n\n\n\nI’ve created an example project folder for running simulations using similar structure from the project organization lecture. Download it here. Unzip and save in an organized location- this will become its own GitHub repository.\nNext, start implementing your simulation study! Write separate scripts for simulating data, applying the method, and extracting estimates.\nFor now, choose just one simulation scenario to implement. If you have time, you can implement others.\n\n\n\nAggregate simulation results. Use informative tables and/or figures to summarize results.\n\n\n\n\n\nTake the following problem from the slides:\nSuppose we have data from a \\(N(\\mu, \\sigma^2)\\) distribution, \\(Y = \\{y_1, \\ldots, y_n\\}\\)\n\nInterested in obtaining an estimate of the standard error of the trimmed mean with 10% trimmed from each tail\nTo employ the parametric bootstrap, we would start by generating \\(n\\) values from a \\(N(\\hat{\\mu}, \\hat{\\sigma}^2)\\)\n\n\\(\\hat{\\mu}, \\hat{\\sigma}^2\\) are the ML estimates\n\nThen, compute trimmed mean using the simulated sample\n\nrepeat B = 10000 times\n\n\n\n\nImplement both the parametric and non-parametric bootstrap for the trimmed mean here. Compare standard error estimates. Which do you expect to perform better? How would you evaluate what is “better”?\n\n\n\nFor the non-parametric bootstrap, construct bootstrap-t confidence intervals for the trimmed mean using B = 1000 for the first level bootstrap and Bt = 500 for the second level bootstrap.\n\n\n\n\nAssume there are two sets of independent normal random variables with the same known variance and different means:\n\n\\(X_i \\sim N(\\mu_1,\\sigma^2)\\)\n\\(Y_i \\sim N(\\mu^2, \\sigma^2)\\)\n\nOur goal is to test \\(H_0: \\mu_1 = \\mu_2\\). Define test statistic: \\(t = \\bar{X}-\\bar{Y}\\). Permutation test steps:\n\nRandomly shuffle labels of \\(X\\) and \\(Y\\)\nCompute \\(t^* = \\bar{X}^*-\\bar{Y}^*\\)\nRepeat nperm times. Resulting \\(t^*\\) values form the empirical null distribution of \\(t\\).\nTo compute p-values calculate \\(Pr(|t^*|&gt; |t|)\\)\n\n\n\n\n\n\n\n\nCreate a function square_for(x) that takes as an argument vector x (of any length), and uses the for loop to return vector y such that \\[y_i = x_i^2\\]\nCreate a function square_sapply(x) that does the same thing using sapply function.\nCreate a function square_vec(x) that does the same thing using ^ operator applied directly to vector x.\nCreate a function square_vec2(x) that does the same thing using * operator applied directly to vector x.\nCreate a vector x of \\(p=100,000\\) normal random variables\n\nNext, use microbenchmark package to compare the performance of the four functions above using x\nWhat’s your conclusion?\n\n\n\nGo back to the lab on simulations. You can start with the solutions file, listed as partial_solution on the canvas site. Parallelize this code."
  },
  {
    "objectID": "content_simulations.html#overview",
    "href": "content_simulations.html#overview",
    "title": "Simulations and resampling methods",
    "section": "",
    "text": "In this, we will cover basic simulations, bootstrapping, permutation tests, and speeding up code through vectorization and parallelization.\n\nSlide Deck: SimulationsSlide Deck: BootstrapSlide Deck: Benchmarking and parallelizationLab: simulationsLab: resampling methodsLab: Benchmarking and parallelization\n\n\n\nSimulation studies\nDownload slides (PDF)\n\n\n\n\nResampling Methods\nDownload slides (PDF)\n\n\n\n\nSpeeding up your code in R\nDownload slides (PDF)\n\n\n\nSimulation study workflow\nFirst, use ADEMP guidelines to define the following\n\nSimulation scenarios\nEstimands/targets\nPerformance measures\n\nThen, for a given simulation scenario follow the basic steps:\n\nChoose \\(n_{sim}\\)\nSimulate data\nApply method(s)\nCalculate estimates\nStore results\n\nWhenever possible, separate scripts for performing different tasks.\n\n\nOur goal in this exercise will be to plan a well-organized simulation study for multiple linear regression. It will be similar in structure to your Homework 2. Below is a multiple linear regression model, where we are interested in primarily treatment effect.\n\\[Y_i = \\beta_0 + \\beta_{treatment}X_{i1} + \\mathbf{Z_i}^T\\boldsymbol{\\gamma} + \\epsilon_i\\]\n\n\\(Y_i\\): continuous outcome\n\\(X_{i1}\\): treatment group indicator; \\(X_{i1}=1\\) for treated\n\\(\\mathbf{Z_i}\\): vector of potential confounders\n\n\n\n\\(\\beta_{treatment}\\): average treatment effect, adjusting for \\(\\mathbf{Z_i}\\)\n\\(\\boldsymbol{\\gamma}\\): vector of regression coefficient values for confounders\n\\(\\epsilon_i\\): iid error, \\(\\sim N(0, \\sigma^2)\\)\n\nIn our simulation, we want to\n\nEstimate \\(\\beta_{treatment}\\)\n\nEvaluate through bias and coverage\n\nEstimate \\(\\sigma^2\\)\n\nEvaluate through bias\n\nEvaluate these properties at:\n\nSample size \\(n \\in \\{10, 50, 500\\}\\)\nTrue values \\(\\beta_{treatment} \\in \\{0.5, 2\\}\\)\nTrue values \\(\\sigma^2 \\in \\{1, 5\\}\\)\n\nAssume for now that there are no confounders (\\(\\boldsymbol{\\gamma} = 0\\))\nAssume \\(X_{i1} = 1\\) with probability 0.5\nUse a full factorial design\n\n\n\n\n\nHow many simulation scenarios will you be running?\nWhat are the estimand(s)\nWhat method(s) are being evaluated/compared?\nWhat are the performance measure(s)?\n\n\n\n\nBased on desired coverage of 90% with Monte Carlo error of no more than 1%, how many simulations (\\(n_{sim}\\)) should we perform for each simulation scenario?\n\n\n\nI’ve created an example project folder for running simulations using similar structure from the project organization lecture. Download it here. Unzip and save in an organized location- this will become its own GitHub repository.\nNext, start implementing your simulation study! Write separate scripts for simulating data, applying the method, and extracting estimates.\nFor now, choose just one simulation scenario to implement. If you have time, you can implement others.\n\n\n\nAggregate simulation results. Use informative tables and/or figures to summarize results.\n\n\n\n\n\nTake the following problem from the slides:\nSuppose we have data from a \\(N(\\mu, \\sigma^2)\\) distribution, \\(Y = \\{y_1, \\ldots, y_n\\}\\)\n\nInterested in obtaining an estimate of the standard error of the trimmed mean with 10% trimmed from each tail\nTo employ the parametric bootstrap, we would start by generating \\(n\\) values from a \\(N(\\hat{\\mu}, \\hat{\\sigma}^2)\\)\n\n\\(\\hat{\\mu}, \\hat{\\sigma}^2\\) are the ML estimates\n\nThen, compute trimmed mean using the simulated sample\n\nrepeat B = 10000 times\n\n\n\n\nImplement both the parametric and non-parametric bootstrap for the trimmed mean here. Compare standard error estimates. Which do you expect to perform better? How would you evaluate what is “better”?\n\n\n\nFor the non-parametric bootstrap, construct bootstrap-t confidence intervals for the trimmed mean using B = 1000 for the first level bootstrap and Bt = 500 for the second level bootstrap.\n\n\n\n\nAssume there are two sets of independent normal random variables with the same known variance and different means:\n\n\\(X_i \\sim N(\\mu_1,\\sigma^2)\\)\n\\(Y_i \\sim N(\\mu^2, \\sigma^2)\\)\n\nOur goal is to test \\(H_0: \\mu_1 = \\mu_2\\). Define test statistic: \\(t = \\bar{X}-\\bar{Y}\\). Permutation test steps:\n\nRandomly shuffle labels of \\(X\\) and \\(Y\\)\nCompute \\(t^* = \\bar{X}^*-\\bar{Y}^*\\)\nRepeat nperm times. Resulting \\(t^*\\) values form the empirical null distribution of \\(t\\).\nTo compute p-values calculate \\(Pr(|t^*|&gt; |t|)\\)\n\n\n\n\n\n\n\n\nCreate a function square_for(x) that takes as an argument vector x (of any length), and uses the for loop to return vector y such that \\[y_i = x_i^2\\]\nCreate a function square_sapply(x) that does the same thing using sapply function.\nCreate a function square_vec(x) that does the same thing using ^ operator applied directly to vector x.\nCreate a function square_vec2(x) that does the same thing using * operator applied directly to vector x.\nCreate a vector x of \\(p=100,000\\) normal random variables\n\nNext, use microbenchmark package to compare the performance of the four functions above using x\nWhat’s your conclusion?\n\n\n\nGo back to the lab on simulations. You can start with the solutions file, listed as partial_solution on the canvas site. Parallelize this code."
  }
]